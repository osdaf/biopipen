{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"biopipen - A set of processes/pipelines for bioinformatics Installation pip install -U biopipen Usage Use as APIs from pipen import Proc , Pipen from biopipen.namespaces.bed import BedLiftOver MyBedLiftOver = Proc . from_proc ( BedLiftOver ) if __name__ == \"__main__\" : Pipen () . set_start ( MyBedLiftOver ) . run () Use as pipen-cli-run plugin \u276f pipen run bed BedLiftOver DESCRIPTION: Liftover a BED file using liftOver USAGE: pipen [ OPTIONS ] OPTIONS FOR <BedLiftOver>: --in.inbed <list> - The input BED file Default: \\[ ] --out.outbed <auto> - The output BED file Default: <awaiting compiling> --envs.liftover <str> - The path to liftOver Default: liftOver --envs.chain <str> - The map chain file for liftover Default: ~/reference/hg38ToHg19.over.chain.gz OPTIONAL OPTIONS: --config <path> - Read options from a configuration file in TOML. Default: None -h, --help - Print help information for this command --full - Show full options for this command PIPELINE OPTIONS: --profile <str> - The default profile from the configuration to run the pipeline. This profile will be used unless a profile is specified in the process or in the .run method of pipen. Default: default --outdir <path> - The output directory of the pipeline Default: ~/bedliftover_pipeline_results/ --workdir <str> - The workdir for the pipeline. Default: ./.pipen --scheduler <str> - The scheduler to run the jobs. Default: local","title":"Home"},{"location":"#biopipen-a-set-of-processespipelines-for-bioinformatics","text":"","title":"biopipen - A set of processes/pipelines for bioinformatics"},{"location":"#installation","text":"pip install -U biopipen","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#use-as-apis","text":"from pipen import Proc , Pipen from biopipen.namespaces.bed import BedLiftOver MyBedLiftOver = Proc . from_proc ( BedLiftOver ) if __name__ == \"__main__\" : Pipen () . set_start ( MyBedLiftOver ) . run ()","title":"Use as APIs"},{"location":"#use-as-pipen-cli-run-plugin","text":"\u276f pipen run bed BedLiftOver DESCRIPTION: Liftover a BED file using liftOver USAGE: pipen [ OPTIONS ] OPTIONS FOR <BedLiftOver>: --in.inbed <list> - The input BED file Default: \\[ ] --out.outbed <auto> - The output BED file Default: <awaiting compiling> --envs.liftover <str> - The path to liftOver Default: liftOver --envs.chain <str> - The map chain file for liftover Default: ~/reference/hg38ToHg19.over.chain.gz OPTIONAL OPTIONS: --config <path> - Read options from a configuration file in TOML. Default: None -h, --help - Print help information for this command --full - Show full options for this command PIPELINE OPTIONS: --profile <str> - The default profile from the configuration to run the pipeline. This profile will be used unless a profile is specified in the process or in the .run method of pipen. Default: default --outdir <path> - The output directory of the pipeline Default: ~/bedliftover_pipeline_results/ --workdir <str> - The workdir for the pipeline. Default: ./.pipen --scheduler <str> - The scheduler to run the jobs. Default: local","title":"Use as pipen-cli-run plugin"},{"location":"CHANGELOG/","text":"0.2.1 User rtoml over toml 0.2.0 \ud83d\udccc Pin deps for docs Don't link non-existing files for misc.Glob2Dir Upgrade datar to 0.8 \u2b06\ufe0f Upgrade pipen to v0.3 \u26a1\ufe0f Load 10X TCR and RNA-seq data files more robustly for scrna.SeuratPreparing and tcr.ImmunarchLoading 0.1.9 \ud83d\udc1b Load all_config_annotations.csv if filtered_contig_annotations.csv doesn't exist for tcr.ImmunarchLoad \ud83d\udc1b Calculate diversity for all clones only if filtering by clone sizes failed for tcr.ImmunarchAdvanced \ud83d\ude91 Fix seurat object creating when expressions are named \"Gene Expression\" for scrna.SeuratPreparing \u2728 Add tcr.TCRClustering \u2728 Add raw to immdata for tcr.immunarchLoading \u2728 Add on_raw env to tcr.TCRClustering \u2728 Add bam.ControlFREEC 0.1.8 \u2728 Add tcr.Attach2Seurat 0.1.7 \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter \ud83d\udc7d\ufe0f Don't wrap job report in report_jobs report macro (to adopt pipen-report 0.2) \u2728 Add more options for scrna.DimPlots 0.1.6 \u2728 Convert CNVpytor results to gff and bed \ud83d\ude91 Make scrna_metabolic pipeline work standalone \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter 0.1.5 \u2728 Add features and fix issues for immunopipe 0.0.4 \u2728 Add some vcf processes 0.1.4 \ud83d\udc1b Fix bam.CNVpytor when snpfile is not provided \u2728 Add metabolic pathway analysis for single-cell RNA-seq data 0.1.3 Add gsea.GSEA and scrna.SCImpute Add gene name conversions Add gsea.FGSEA Add venn plots and refactor ImmunarchFilter Add plot.Heatmap Reuse plot.Heatmap for scrna.GeneExpressionInvestigation Attach metadata to seurat object in scrna.SeuratPreparing Add envs.group_subset for scrna.GeneExpressionInvestigation Fix typo for scrna.GeneExpressionInvestigation Add docs 0.1.2 \u2728 Add envs.qc for scrna.SeuratPreparing 0.1.1 Finish processes for immunopipe 0.1.0 Adopt pipen 0.2+","title":"Change log"},{"location":"CHANGELOG/#021","text":"User rtoml over toml","title":"0.2.1"},{"location":"CHANGELOG/#020","text":"\ud83d\udccc Pin deps for docs Don't link non-existing files for misc.Glob2Dir Upgrade datar to 0.8 \u2b06\ufe0f Upgrade pipen to v0.3 \u26a1\ufe0f Load 10X TCR and RNA-seq data files more robustly for scrna.SeuratPreparing and tcr.ImmunarchLoading","title":"0.2.0"},{"location":"CHANGELOG/#019","text":"\ud83d\udc1b Load all_config_annotations.csv if filtered_contig_annotations.csv doesn't exist for tcr.ImmunarchLoad \ud83d\udc1b Calculate diversity for all clones only if filtering by clone sizes failed for tcr.ImmunarchAdvanced \ud83d\ude91 Fix seurat object creating when expressions are named \"Gene Expression\" for scrna.SeuratPreparing \u2728 Add tcr.TCRClustering \u2728 Add raw to immdata for tcr.immunarchLoading \u2728 Add on_raw env to tcr.TCRClustering \u2728 Add bam.ControlFREEC","title":"0.1.9"},{"location":"CHANGELOG/#018","text":"\u2728 Add tcr.Attach2Seurat","title":"0.1.8"},{"location":"CHANGELOG/#017","text":"\u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter \ud83d\udc7d\ufe0f Don't wrap job report in report_jobs report macro (to adopt pipen-report 0.2) \u2728 Add more options for scrna.DimPlots","title":"0.1.7"},{"location":"CHANGELOG/#016","text":"\u2728 Convert CNVpytor results to gff and bed \ud83d\ude91 Make scrna_metabolic pipeline work standalone \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter","title":"0.1.6"},{"location":"CHANGELOG/#015","text":"\u2728 Add features and fix issues for immunopipe 0.0.4 \u2728 Add some vcf processes","title":"0.1.5"},{"location":"CHANGELOG/#014","text":"\ud83d\udc1b Fix bam.CNVpytor when snpfile is not provided \u2728 Add metabolic pathway analysis for single-cell RNA-seq data","title":"0.1.4"},{"location":"CHANGELOG/#013","text":"Add gsea.GSEA and scrna.SCImpute Add gene name conversions Add gsea.FGSEA Add venn plots and refactor ImmunarchFilter Add plot.Heatmap Reuse plot.Heatmap for scrna.GeneExpressionInvestigation Attach metadata to seurat object in scrna.SeuratPreparing Add envs.group_subset for scrna.GeneExpressionInvestigation Fix typo for scrna.GeneExpressionInvestigation Add docs","title":"0.1.3"},{"location":"CHANGELOG/#012","text":"\u2728 Add envs.qc for scrna.SeuratPreparing","title":"0.1.2"},{"location":"CHANGELOG/#011","text":"Finish processes for immunopipe","title":"0.1.1"},{"location":"CHANGELOG/#010","text":"Adopt pipen 0.2+","title":"0.1.0"},{"location":"api/biopipen.core.config/","text":"module biopipen.core . config </> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> class biopipen.core.config . ConfigItems ( *args , **kwargs ) </> Bases diot.diot.Diot dict Provides the envs from configuration files and defaults thenon-existing values to None. Parameters *args \u2014 Anything that can be sent to dict construct **kwargs \u2014 keyword argument that can be sent to dict constructSome diot configurations can also be passed, including: diot_nest: Types to nestly convert values diot_transform: The transforms for keys diot_frozen: Whether to generate a frozen diot. True: freeze the object recursively if there are Diot objects in descendants False: Don'f freeze 'shallow': Only freeze at depth = 1 Methods __contains__ ( name ) (bool) \u2014 True if the dictionary has the specified key, else False. </> __delitem__ ( name ) \u2014 Delete self[key]. </> __getitem__ ( name ) (any) \u2014 x. getitem (y) <==> x[y] </> __ior__ ( other ) (Diot) \u2014 Return self|=value. </> __or__ ( other ) (Diot) \u2014 Return self|value. </> __setitem__ ( name , value ) \u2014 Set self[key] to value. </> accessible_keys ( ) (iterable of str) \u2014 Get the converted keys </> clear ( ) \u2014 Clear the object </> copy ( ) (Diot) \u2014 Shallow copy the object </> freeze ( frozen ) \u2014 Freeze the diot object </> from_namespace ( namespace , recursive , diot_nest , diot_transform , diot_frozen ) (Diot) \u2014 Get a Diot object from an argparse namespace </> get ( name , value ) (any) \u2014 Get the value of a key name </> pop ( name , *value ) (any) \u2014 Pop a key from the object and return the value. If key does notexist, return the given default value </> popitem ( ) (str, any) \u2014 Pop last item from the object </> setdefault ( name , value ) (any) \u2014 Set a default value to a key </> thaw ( recursive ) \u2014 A context manager for temporarily change the diot </> to_dict ( ) (dict(str: any)) \u2014 Turn the Box and sub Boxes back into a nativepython dictionary. </> to_json ( filename , encoding , errors , **json_kwargs ) (str, optional) \u2014 Convert to a json string or save it to json file </> to_toml ( filename , encoding , errors ) (str, optional) \u2014 Convert to a toml string or save it to toml file </> to_yaml ( filename , default_flow_style , encoding , errors , **yaml_kwargs ) (str, optional) \u2014 Convert to a yaml string or save it to yaml file </> unfreeze ( recursive ) \u2014 Unfreeze the diot object </> update ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> classmethod from_namespace ( namespace , recursive=True , diot_nest=True , diot_transform='safe' , diot_frozen=False ) </> Get a Diot object from an argparse namespace Example >>> from argparse import Namespace >>> Diot . from_namespace ( Namespace ( a = 1 , b = 2 )) Parameters namespace (Namespace) \u2014 The namespace object recursive (bool, optional) \u2014 Do it recursively? diot_nest (Union(bool, iterable of type), optional) \u2014 Types to nestly convert values diot_transform (Union(callable(str: str), str), optional) \u2014 The transforms for keys diot_frozen (bool or str, optional) \u2014 Whether to generate a frozen diot. - True: freeze the object recursively if there are Diot objects in descendants - False: Don'f freeze - shallow : Only freeze at depth = 1 diot_missing \u2014 How to deal with missing keys when accessing them - An exception class or object to raise - None to return None - A custom function with first argument the key and second the diot object. Returns (Diot) The converted diot object. method __setitem__ ( name , value ) </> Set self[key] to value. method pop ( name , *value ) </> Pop a key from the object and return the value. If key does notexist, return the given default value Parameters name (str) \u2014 The key Returns (any) The value corresponding to the name or the default value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method popitem ( ) </> Pop last item from the object Returns (str, any) A tuple of key and value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method update ( *value , **kwargs ) </> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method __or__ ( other ) \u2192 Diot </> Return self|value. method __ior__ ( other ) \u2192 Diot </> Return self|=value. method __delitem__ ( name ) </> Delete self[key]. method freeze ( frozen='shallow' ) </> Freeze the diot object Parameters frozen (str or bool, optional) \u2014 The frozen argument indicating how to freeze:shallow: only freeze at depth=1 True: freeze recursively if there are diot objects in children False: Disable freezing method unfreeze ( recursive=False ) </> Unfreeze the diot object Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively generator thaw ( recursive=False ) </> A context manager for temporarily change the diot Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively Yields self, the reference to this diot. method setdefault ( name , value ) </> Set a default value to a key Parameters name (str) \u2014 The key name value (any) \u2014 The default value Returns (any) The existing value or the value passed in Raises DiotFrozenError \u2014 when try to set default to a frozen diot method accessible_keys ( ) </> Get the converted keys Returns (iterable of str) The accessible (transformed) keys method get ( name , value=None ) </> Get the value of a key name Parameters name (str) \u2014 The key name value (any, optional) \u2014 The value to return if the key does not exist Returns (any) The corresponding value or the value passed in if the key doesnot exist method __contains__ ( name ) \u2192 bool </> True if the dictionary has the specified key, else False. method clear ( ) </> Clear the object method copy ( ) </> Shallow copy the object Returns (Diot) The copied object method to_dict ( ) </> Turn the Box and sub Boxes back into a nativepython dictionary. Returns (dict(str: any)) The converted python dictionary method to_json ( filename=None , encoding='utf-8' , errors='strict' , **json_kwargs ) </> Convert to a json string or save it to json file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the json to, if not given a jsonstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **json_kwargs \u2014 Other kwargs for json.dumps Returns (str, optional) The json string with filename is not given method to_yaml ( filename=None , default_flow_style=False , encoding='utf-8' , errors='strict' , **yaml_kwargs ) </> Convert to a yaml string or save it to yaml file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the yaml to, if not given a yamlstring will be returned default_flow_style (bool, optional) \u2014 The default flow style for yaml dumpingSee yaml.dump encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **yaml_kwargs \u2014 Other kwargs for yaml.dump Returns (str, optional) The yaml string with filename is not given method to_toml ( filename=None , encoding='utf-8' , errors='strict' ) </> Convert to a toml string or save it to toml file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the toml to, if not given a tomlstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function Returns (str, optional) The toml string with filename is not given method __getitem__ ( name ) \u2192 any </> x. getitem (y) <==> x[y]","title":"biopipen.core.config"},{"location":"api/biopipen.core.config/#biopipencoreconfig","text":"</> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> class","title":"biopipen.core.config"},{"location":"api/biopipen.core.config/#biopipencoreconfigconfigitems","text":"</> Bases diot.diot.Diot dict Provides the envs from configuration files and defaults thenon-existing values to None. Parameters *args \u2014 Anything that can be sent to dict construct **kwargs \u2014 keyword argument that can be sent to dict constructSome diot configurations can also be passed, including: diot_nest: Types to nestly convert values diot_transform: The transforms for keys diot_frozen: Whether to generate a frozen diot. True: freeze the object recursively if there are Diot objects in descendants False: Don'f freeze 'shallow': Only freeze at depth = 1 Methods __contains__ ( name ) (bool) \u2014 True if the dictionary has the specified key, else False. </> __delitem__ ( name ) \u2014 Delete self[key]. </> __getitem__ ( name ) (any) \u2014 x. getitem (y) <==> x[y] </> __ior__ ( other ) (Diot) \u2014 Return self|=value. </> __or__ ( other ) (Diot) \u2014 Return self|value. </> __setitem__ ( name , value ) \u2014 Set self[key] to value. </> accessible_keys ( ) (iterable of str) \u2014 Get the converted keys </> clear ( ) \u2014 Clear the object </> copy ( ) (Diot) \u2014 Shallow copy the object </> freeze ( frozen ) \u2014 Freeze the diot object </> from_namespace ( namespace , recursive , diot_nest , diot_transform , diot_frozen ) (Diot) \u2014 Get a Diot object from an argparse namespace </> get ( name , value ) (any) \u2014 Get the value of a key name </> pop ( name , *value ) (any) \u2014 Pop a key from the object and return the value. If key does notexist, return the given default value </> popitem ( ) (str, any) \u2014 Pop last item from the object </> setdefault ( name , value ) (any) \u2014 Set a default value to a key </> thaw ( recursive ) \u2014 A context manager for temporarily change the diot </> to_dict ( ) (dict(str: any)) \u2014 Turn the Box and sub Boxes back into a nativepython dictionary. </> to_json ( filename , encoding , errors , **json_kwargs ) (str, optional) \u2014 Convert to a json string or save it to json file </> to_toml ( filename , encoding , errors ) (str, optional) \u2014 Convert to a toml string or save it to toml file </> to_yaml ( filename , default_flow_style , encoding , errors , **yaml_kwargs ) (str, optional) \u2014 Convert to a yaml string or save it to yaml file </> unfreeze ( recursive ) \u2014 Unfreeze the diot object </> update ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> classmethod","title":"biopipen.core.config.ConfigItems"},{"location":"api/biopipen.core.config/#diotdiotdiotfrom_namespace","text":"</> Get a Diot object from an argparse namespace Example >>> from argparse import Namespace >>> Diot . from_namespace ( Namespace ( a = 1 , b = 2 )) Parameters namespace (Namespace) \u2014 The namespace object recursive (bool, optional) \u2014 Do it recursively? diot_nest (Union(bool, iterable of type), optional) \u2014 Types to nestly convert values diot_transform (Union(callable(str: str), str), optional) \u2014 The transforms for keys diot_frozen (bool or str, optional) \u2014 Whether to generate a frozen diot. - True: freeze the object recursively if there are Diot objects in descendants - False: Don'f freeze - shallow : Only freeze at depth = 1 diot_missing \u2014 How to deal with missing keys when accessing them - An exception class or object to raise - None to return None - A custom function with first argument the key and second the diot object. Returns (Diot) The converted diot object. method","title":"diot.diot.Diot.from_namespace"},{"location":"api/biopipen.core.config/#diotdiotdiotsetitem","text":"</> Set self[key] to value. method","title":"diot.diot.Diot.setitem"},{"location":"api/biopipen.core.config/#diotdiotdiotpop","text":"</> Pop a key from the object and return the value. If key does notexist, return the given default value Parameters name (str) \u2014 The key Returns (any) The value corresponding to the name or the default value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method","title":"diot.diot.Diot.pop"},{"location":"api/biopipen.core.config/#diotdiotdiotpopitem","text":"</> Pop last item from the object Returns (str, any) A tuple of key and value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method","title":"diot.diot.Diot.popitem"},{"location":"api/biopipen.core.config/#diotdiotdiotupdate","text":"</> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method","title":"diot.diot.Diot.update"},{"location":"api/biopipen.core.config/#diotdiotdiotor","text":"</> Return self|value. method","title":"diot.diot.Diot.or"},{"location":"api/biopipen.core.config/#diotdiotdiotior","text":"</> Return self|=value. method","title":"diot.diot.Diot.ior"},{"location":"api/biopipen.core.config/#diotdiotdiotdelitem","text":"</> Delete self[key]. method","title":"diot.diot.Diot.delitem"},{"location":"api/biopipen.core.config/#diotdiotdiotfreeze","text":"</> Freeze the diot object Parameters frozen (str or bool, optional) \u2014 The frozen argument indicating how to freeze:shallow: only freeze at depth=1 True: freeze recursively if there are diot objects in children False: Disable freezing method","title":"diot.diot.Diot.freeze"},{"location":"api/biopipen.core.config/#diotdiotdiotunfreeze","text":"</> Unfreeze the diot object Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively generator","title":"diot.diot.Diot.unfreeze"},{"location":"api/biopipen.core.config/#diotdiotdiotthaw","text":"</> A context manager for temporarily change the diot Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively Yields self, the reference to this diot. method","title":"diot.diot.Diot.thaw"},{"location":"api/biopipen.core.config/#diotdiotdiotsetdefault","text":"</> Set a default value to a key Parameters name (str) \u2014 The key name value (any) \u2014 The default value Returns (any) The existing value or the value passed in Raises DiotFrozenError \u2014 when try to set default to a frozen diot method","title":"diot.diot.Diot.setdefault"},{"location":"api/biopipen.core.config/#diotdiotdiotaccessible_keys","text":"</> Get the converted keys Returns (iterable of str) The accessible (transformed) keys method","title":"diot.diot.Diot.accessible_keys"},{"location":"api/biopipen.core.config/#diotdiotdiotget","text":"</> Get the value of a key name Parameters name (str) \u2014 The key name value (any, optional) \u2014 The value to return if the key does not exist Returns (any) The corresponding value or the value passed in if the key doesnot exist method","title":"diot.diot.Diot.get"},{"location":"api/biopipen.core.config/#diotdiotdiotcontains","text":"</> True if the dictionary has the specified key, else False. method","title":"diot.diot.Diot.contains"},{"location":"api/biopipen.core.config/#diotdiotdiotclear","text":"</> Clear the object method","title":"diot.diot.Diot.clear"},{"location":"api/biopipen.core.config/#diotdiotdiotcopy","text":"</> Shallow copy the object Returns (Diot) The copied object method","title":"diot.diot.Diot.copy"},{"location":"api/biopipen.core.config/#diotdiotdiotto_dict","text":"</> Turn the Box and sub Boxes back into a nativepython dictionary. Returns (dict(str: any)) The converted python dictionary method","title":"diot.diot.Diot.to_dict"},{"location":"api/biopipen.core.config/#diotdiotdiotto_json","text":"</> Convert to a json string or save it to json file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the json to, if not given a jsonstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **json_kwargs \u2014 Other kwargs for json.dumps Returns (str, optional) The json string with filename is not given method","title":"diot.diot.Diot.to_json"},{"location":"api/biopipen.core.config/#diotdiotdiotto_yaml","text":"</> Convert to a yaml string or save it to yaml file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the yaml to, if not given a yamlstring will be returned default_flow_style (bool, optional) \u2014 The default flow style for yaml dumpingSee yaml.dump encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **yaml_kwargs \u2014 Other kwargs for yaml.dump Returns (str, optional) The yaml string with filename is not given method","title":"diot.diot.Diot.to_yaml"},{"location":"api/biopipen.core.config/#diotdiotdiotto_toml","text":"</> Convert to a toml string or save it to toml file Parameters filename (str, PathLike, or NoneType, optional) \u2014 The filename to save the toml to, if not given a tomlstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function Returns (str, optional) The toml string with filename is not given method","title":"diot.diot.Diot.to_toml"},{"location":"api/biopipen.core.config/#biopipencoreconfigconfigitemsgetitem","text":"</> x. getitem (y) <==> x[y]","title":"biopipen.core.config.ConfigItems.getitem"},{"location":"api/biopipen.core.defaults/","text":"module biopipen.core . defaults </> Provide default settgins","title":"biopipen.core.defaults"},{"location":"api/biopipen.core.defaults/#biopipencoredefaults","text":"</> Provide default settgins","title":"biopipen.core.defaults"},{"location":"api/biopipen.core/","text":"package biopipen. core </> module biopipen.core . defaults </> Provide default settgins module biopipen.core . config </> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> module biopipen.core . proc </> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </>","title":"biopipen.core"},{"location":"api/biopipen.core/#biopipencore","text":"</> module","title":"biopipen.core"},{"location":"api/biopipen.core/#biopipencoredefaults","text":"</> Provide default settgins module","title":"biopipen.core.defaults"},{"location":"api/biopipen.core/#biopipencoreconfig","text":"</> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> module","title":"biopipen.core.config"},{"location":"api/biopipen.core/#biopipencoreproc","text":"</> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </>","title":"biopipen.core.proc"},{"location":"api/biopipen.core.proc/","text":"module biopipen.core . proc </> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class biopipen.core.proc . Proc ( *args , **kwds ) \u2192 Proc </> Bases pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.core.proc"},{"location":"api/biopipen.core.proc/#biopipencoreproc","text":"</> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class","title":"biopipen.core.proc"},{"location":"api/biopipen.core.proc/#biopipencoreprocproc","text":"</> Bases pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.core.proc.Proc"},{"location":"api/biopipen.core.proc/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.core.proc/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.core.proc/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.core.proc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.core.proc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.core.proc/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.bam/","text":"module biopipen.namespaces . bam </> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> class biopipen.namespaces.bam . CNVpytor ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNV using CNVpytor Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: cnvpytor: Path to cnvpytor ncores: Number of cores to use ( -j for cnvpytor) cases: Cases to run cnvpytor. A dictionary with keys as case names and values is also a dict - chrom : The chromosomes to run on - binsizes : The binsizes - snp : How to read snp data - mask_snps : Whether mask 1000 Genome snps - baf_nomask : Do not use P mask in BAF histograms Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.bam . ControlFREEC ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using Control-FREEC Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: freec: Path to Control-FREEC executable ncores: Number of cores to use arggs: Other arguments for Control-FREEC Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.bam"},{"location":"api/biopipen.namespaces.bam/#biopipennamespacesbam","text":"</> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> class","title":"biopipen.namespaces.bam"},{"location":"api/biopipen.namespaces.bam/#biopipennamespacesbamcnvpytor","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNV using CNVpytor Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: cnvpytor: Path to cnvpytor ncores: Number of cores to use ( -j for cnvpytor) cases: Cases to run cnvpytor. A dictionary with keys as case names and values is also a dict - chrom : The chromosomes to run on - binsizes : The binsizes - snp : How to read snp data - mask_snps : Whether mask 1000 Genome snps - baf_nomask : Do not use P mask in BAF histograms Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.bam.CNVpytor"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.bam/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.bam/#biopipennamespacesbamcontrolfreec","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using Control-FREEC Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: freec: Path to Control-FREEC executable ncores: Number of cores to use arggs: Other arguments for Control-FREEC Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.bam.ControlFREEC"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.bam/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.bam/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.bcftools/","text":"module biopipen.namespaces . bcftools </> handling VCF files using bcftools Classes BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> class biopipen.namespaces.bcftools . BcftoolsAnnotate ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Add or remove annotations from VCF files Input: infile: The input VCF file annfile: The annotation file Output: outfile: The annotated VCF file Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile and annfile annfile: The annotation file. If in.annfile is provided, this is ignored ncores: Number of cores ( --nthread ) to use cols: Overwrite -c/--columns header: Headers to be added args: Other arguments for bcftools annotate Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.bcftools . BcftoolsFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Apply fixed threshold filters to VCF files Input: infile: The input VCF file Output: outfile: The filtered VCF file. If the in.infile is gzipped, this is gzipped as well. Envs: bcftools: Path to bcftools ncores: Number of cores ( --nthread ) to use keep: Whether we should keep the filtered variants or not. args: Other arguments for bcftools annotate ncores: nthread tmpdir: Path to save the intermediate files Since the filters need to be applied one by one by bcftools includes: and excludes: include/exclude only sites for which EXPRESSION is true. - See: https://samtools.github.io/bcftools/bcftools.html#expressions - If provided, envs.args.include/exclude will be ignored. - If str / list used, The filter names will be Filter%d - A dict is used when keys are filter names and values are expressions Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.bcftools"},{"location":"api/biopipen.namespaces.bcftools/#biopipennamespacesbcftools","text":"</> handling VCF files using bcftools Classes BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> class","title":"biopipen.namespaces.bcftools"},{"location":"api/biopipen.namespaces.bcftools/#biopipennamespacesbcftoolsbcftoolsannotate","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Add or remove annotations from VCF files Input: infile: The input VCF file annfile: The annotation file Output: outfile: The annotated VCF file Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile and annfile annfile: The annotation file. If in.annfile is provided, this is ignored ncores: Number of cores ( --nthread ) to use cols: Overwrite -c/--columns header: Headers to be added args: Other arguments for bcftools annotate Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.bcftools.BcftoolsAnnotate"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.bcftools/#biopipennamespacesbcftoolsbcftoolsfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Apply fixed threshold filters to VCF files Input: infile: The input VCF file Output: outfile: The filtered VCF file. If the in.infile is gzipped, this is gzipped as well. Envs: bcftools: Path to bcftools ncores: Number of cores ( --nthread ) to use keep: Whether we should keep the filtered variants or not. args: Other arguments for bcftools annotate ncores: nthread tmpdir: Path to save the intermediate files Since the filters need to be applied one by one by bcftools includes: and excludes: include/exclude only sites for which EXPRESSION is true. - See: https://samtools.github.io/bcftools/bcftools.html#expressions - If provided, envs.args.include/exclude will be ignored. - If str / list used, The filter names will be Filter%d - A dict is used when keys are filter names and values are expressions Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.bcftools.BcftoolsFilter"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.bcftools/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.bed/","text":"module biopipen.namespaces . bed </> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> class biopipen.namespaces.bed . BedLiftOver ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a BED file using liftOver Input: inbed: The input BED file Output: outbed: The output BED file Envs: liftover: The path to liftOver chain: The map chain file for liftover Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.bed"},{"location":"api/biopipen.namespaces.bed/#biopipennamespacesbed","text":"</> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> class","title":"biopipen.namespaces.bed"},{"location":"api/biopipen.namespaces.bed/#biopipennamespacesbedbedliftover","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a BED file using liftOver Input: inbed: The input BED file Output: outbed: The output BED file Envs: liftover: The path to liftOver chain: The map chain file for liftover Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.bed.BedLiftOver"},{"location":"api/biopipen.namespaces.bed/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.bed/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.bed/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.bed/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.bed/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.bed/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.csv/","text":"module biopipen.namespaces . csv </> Tools to deal with csv/tsv files Classes BindRows ( Proc ) \u2014 Bind rows of input files </> class biopipen.namespaces.csv . BindRows ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Bind rows of input files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.csv"},{"location":"api/biopipen.namespaces.csv/#biopipennamespacescsv","text":"</> Tools to deal with csv/tsv files Classes BindRows ( Proc ) \u2014 Bind rows of input files </> class","title":"biopipen.namespaces.csv"},{"location":"api/biopipen.namespaces.csv/#biopipennamespacescsvbindrows","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Bind rows of input files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.csv.BindRows"},{"location":"api/biopipen.namespaces.csv/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.csv/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.csv/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.csv/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.csv/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.csv/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.gene/","text":"module biopipen.namespaces . gene </> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> class biopipen.namespaces.gene . GeneNameConversion ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert gene names back and forth using MyGeneInfo Input: infile: The input file with original gene names Output: outfile: The output file with converted gene names Envs: inopts: Options to read in.infile for pandas.read_csv() See https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html outopts: Options to write out.outfile for pandas.to_csv() See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html notfound: What to do if a conversion cannot be done. use-query: Ignore the conversion and use the original name skip: Ignore the conversion and skip the entire row in input file error: Report error genecol: The index (0-based) or name of the column where genes are present output: How to output keep: Keep the original name column and add new converted columns drop: Drop the original name column, and add the converted names replace: Drop the original name column, and insert the converted names at the original position only: Only keep the query and the converted name columns infmt: What's the original gene name format Available fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields outfmt: What's the target gene name format species: Limit gene query to certain species. Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.gene"},{"location":"api/biopipen.namespaces.gene/#biopipennamespacesgene","text":"</> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> class","title":"biopipen.namespaces.gene"},{"location":"api/biopipen.namespaces.gene/#biopipennamespacesgenegenenameconversion","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert gene names back and forth using MyGeneInfo Input: infile: The input file with original gene names Output: outfile: The output file with converted gene names Envs: inopts: Options to read in.infile for pandas.read_csv() See https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html outopts: Options to write out.outfile for pandas.to_csv() See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html notfound: What to do if a conversion cannot be done. use-query: Ignore the conversion and use the original name skip: Ignore the conversion and skip the entire row in input file error: Report error genecol: The index (0-based) or name of the column where genes are present output: How to output keep: Keep the original name column and add new converted columns drop: Drop the original name column, and add the converted names replace: Drop the original name column, and insert the converted names at the original position only: Only keep the query and the converted name columns infmt: What's the original gene name format Available fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields outfmt: What's the target gene name format species: Limit gene query to certain species. Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.gene.GeneNameConversion"},{"location":"api/biopipen.namespaces.gene/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.gene/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.gene/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.gene/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.gene/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.gene/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.gsea/","text":"module biopipen.namespaces . gsea </> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> class biopipen.namespaces.gsea . GSEA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis Need devtools::install_github(\"GSEA-MSigDB/GSEA_R\") Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol doc.string : Documentation string used as a prefix to name result files. If not provided, will use envs['doc.string'] Output: outdir: The output directory Envs: inopts: The options for read.table() to read the input file If rds will use readRDS() metaopts: The options for read.table() to read the meta file clscol: The column of the metafile determining the classes doc.string: Documentation string used as a prefix to name result files Other configs passed to GSEA() directly Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.gsea . PreRank ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc PreRank the genes for GSEA analysis Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . Output: outfile: The rank file with 1st column the genes, and the rest the ranks for different class pairs provided by envs.classes or in.configfile Envs: inopts: Options for read.table() to read in.infile metaopts: Options for read.table() to read in.metafile method: The method to do the preranking. Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.gsea . FGSEA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using fgsea Need devtools::install_github(\"ctlab/fgsea\") Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . Output: outdir: The output directory Envs: inopts: The options for read.table() to read the input file If rds will use readRDS() metaopts: The options for read.table() to read the meta file method: The method to do the preranking. Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) top: Do gsea table and enrich plot for top N pathways. If it is < 1, will apply it to padj <rest> : Rest arguments for fgsea() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.gsea . Enrichr ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using Enrichr Need devtools::install_github(\"wjawaid/enrichR\") Input: infile: The gene list file. You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output: outdir: The output directory Envs: inopts: Options for read.table() to read in.infile genecol: Which column has the genes (0-based index or column name) dbs: The databases to enrich against. See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.gsea"},{"location":"api/biopipen.namespaces.gsea/#biopipennamespacesgsea","text":"</> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> class","title":"biopipen.namespaces.gsea"},{"location":"api/biopipen.namespaces.gsea/#biopipennamespacesgseagsea","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis Need devtools::install_github(\"GSEA-MSigDB/GSEA_R\") Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol doc.string : Documentation string used as a prefix to name result files. If not provided, will use envs['doc.string'] Output: outdir: The output directory Envs: inopts: The options for read.table() to read the input file If rds will use readRDS() metaopts: The options for read.table() to read the meta file clscol: The column of the metafile determining the classes doc.string: Documentation string used as a prefix to name result files Other configs passed to GSEA() directly Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.gsea.GSEA"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.gsea/#biopipennamespacesgseaprerank","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc PreRank the genes for GSEA analysis Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . Output: outfile: The rank file with 1st column the genes, and the rest the ranks for different class pairs provided by envs.classes or in.configfile Envs: inopts: Options for read.table() to read in.infile metaopts: Options for read.table() to read in.metafile method: The method to do the preranking. Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.gsea.PreRank"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.gsea/#biopipennamespacesgseafgsea","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using fgsea Need devtools::install_github(\"ctlab/fgsea\") Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . Output: outdir: The output directory Envs: inopts: The options for read.table() to read the input file If rds will use readRDS() metaopts: The options for read.table() to read the meta file method: The method to do the preranking. Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) top: Do gsea table and enrich plot for top N pathways. If it is < 1, will apply it to padj <rest> : Rest arguments for fgsea() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.gsea.FGSEA"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.gsea/#biopipennamespacesgseaenrichr","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using Enrichr Need devtools::install_github(\"wjawaid/enrichR\") Input: infile: The gene list file. You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output: outdir: The output directory Envs: inopts: Options for read.table() to read in.infile genecol: Which column has the genes (0-based index or column name) dbs: The databases to enrich against. See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.gsea.Enrichr"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.gsea/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.gsea/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces/","text":"package biopipen. namespaces </> module biopipen.namespaces . tcr </> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> ImmunarchBasic ( Proc ) \u2014 Immunarch - Basic statistics and clonality </> ImmunarchAdvanced ( Proc ) \u2014 Immunarch - Advanced analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> module biopipen.namespaces . gsea </> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> module biopipen.namespaces . rnaseq </> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> module biopipen.namespaces . scrna_metabolic </> Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape If you have clustering done somewhere else, you could use replace_clustering() See docs here https://pwwang.github.io/biopipen/pipelines/scrna_metabolic Reference: [1] Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Start Process: MetabolicInputs Functions build_processes ( options ) \u2014 Build processes for metabolic landscape analysis pipeline </> metabolic_landscape ( ) (Pipen) \u2014 Build a pipeline for pipen run to run </> module biopipen.namespaces . vcf </> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> module biopipen.namespaces . web </> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> module biopipen.namespaces . misc </> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> module biopipen.namespaces . bed </> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> module biopipen.namespaces . gene </> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> module biopipen.namespaces . plot </> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> module biopipen.namespaces . bcftools </> handling VCF files using bcftools Classes BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> module biopipen.namespaces . bam </> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> module biopipen.namespaces . scrna </> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Seurat - Loading and preparing data </> SeuratClustering ( Proc ) \u2014 Seurat - Determine the clusters </> GeneExpressionInvestigation ( Proc ) \u2014 Investigation of expressions of genes of interest </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> module biopipen.namespaces . csv </> Tools to deal with csv/tsv files Classes BindRows ( Proc ) \u2014 Bind rows of input files </>","title":"biopipen.namespaces"},{"location":"api/biopipen.namespaces/#biopipennamespaces","text":"</> module","title":"biopipen.namespaces"},{"location":"api/biopipen.namespaces/#biopipennamespacestcr","text":"</> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> ImmunarchBasic ( Proc ) \u2014 Immunarch - Basic statistics and clonality </> ImmunarchAdvanced ( Proc ) \u2014 Immunarch - Advanced analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> module","title":"biopipen.namespaces.tcr"},{"location":"api/biopipen.namespaces/#biopipennamespacesgsea","text":"</> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> module","title":"biopipen.namespaces.gsea"},{"location":"api/biopipen.namespaces/#biopipennamespacesrnaseq","text":"</> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> module","title":"biopipen.namespaces.rnaseq"},{"location":"api/biopipen.namespaces/#biopipennamespacesscrna_metabolic","text":"</> Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape If you have clustering done somewhere else, you could use replace_clustering() See docs here https://pwwang.github.io/biopipen/pipelines/scrna_metabolic Reference: [1] Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Start Process: MetabolicInputs Functions build_processes ( options ) \u2014 Build processes for metabolic landscape analysis pipeline </> metabolic_landscape ( ) (Pipen) \u2014 Build a pipeline for pipen run to run </> module","title":"biopipen.namespaces.scrna_metabolic"},{"location":"api/biopipen.namespaces/#biopipennamespacesvcf","text":"</> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> module","title":"biopipen.namespaces.vcf"},{"location":"api/biopipen.namespaces/#biopipennamespacesweb","text":"</> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> module","title":"biopipen.namespaces.web"},{"location":"api/biopipen.namespaces/#biopipennamespacesmisc","text":"</> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> module","title":"biopipen.namespaces.misc"},{"location":"api/biopipen.namespaces/#biopipennamespacesbed","text":"</> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> module","title":"biopipen.namespaces.bed"},{"location":"api/biopipen.namespaces/#biopipennamespacesgene","text":"</> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> module","title":"biopipen.namespaces.gene"},{"location":"api/biopipen.namespaces/#biopipennamespacesplot","text":"</> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> module","title":"biopipen.namespaces.plot"},{"location":"api/biopipen.namespaces/#biopipennamespacesbcftools","text":"</> handling VCF files using bcftools Classes BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> module","title":"biopipen.namespaces.bcftools"},{"location":"api/biopipen.namespaces/#biopipennamespacesbam","text":"</> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> module","title":"biopipen.namespaces.bam"},{"location":"api/biopipen.namespaces/#biopipennamespacesscrna","text":"</> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Seurat - Loading and preparing data </> SeuratClustering ( Proc ) \u2014 Seurat - Determine the clusters </> GeneExpressionInvestigation ( Proc ) \u2014 Investigation of expressions of genes of interest </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> module","title":"biopipen.namespaces.scrna"},{"location":"api/biopipen.namespaces/#biopipennamespacescsv","text":"</> Tools to deal with csv/tsv files Classes BindRows ( Proc ) \u2014 Bind rows of input files </>","title":"biopipen.namespaces.csv"},{"location":"api/biopipen.namespaces.misc/","text":"module biopipen.namespaces . misc </> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> class biopipen.namespaces.misc . File2Proc ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Accept a file and pass it down with a symbolic link Input: infile: The input file Output: outfile: The output symbolic link to the input file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.misc . Glob2Dir ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Create symbolic links in output directory for the files givenby the glob pattern Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.misc . Config2File ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a configurationn in string to a configuration file Requires python package rtoml Input: config: A string representation of configuration name: The name for output file. Will be config if not given Output: outfile: The output file with the configuration Envs: infmt: The input format. json or toml . outfmt: The output format. json or toml . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.misc . Str2File ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Write the given string to a file Input: str: The string to write to file name: The name of the file If not given, use envs.name Output: outfile: The output file Envs: name: The name of the output file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.misc"},{"location":"api/biopipen.namespaces.misc/#biopipennamespacesmisc","text":"</> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> class","title":"biopipen.namespaces.misc"},{"location":"api/biopipen.namespaces.misc/#biopipennamespacesmiscfile2proc","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Accept a file and pass it down with a symbolic link Input: infile: The input file Output: outfile: The output symbolic link to the input file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.misc.File2Proc"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.misc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.misc/#biopipennamespacesmiscglob2dir","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Create symbolic links in output directory for the files givenby the glob pattern Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.misc.Glob2Dir"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.misc/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.misc/#biopipennamespacesmiscconfig2file","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a configurationn in string to a configuration file Requires python package rtoml Input: config: A string representation of configuration name: The name for output file. Will be config if not given Output: outfile: The output file with the configuration Envs: infmt: The input format. json or toml . outfmt: The output format. json or toml . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.misc.Config2File"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.misc/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.misc/#biopipennamespacesmiscstr2file","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Write the given string to a file Input: str: The string to write to file name: The name of the file If not given, use envs.name Output: outfile: The output file Envs: name: The name of the output file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.misc.Str2File"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.misc/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.misc/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.plot/","text":"module biopipen.namespaces . plot </> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> class biopipen.namespaces.plot . VennDiagram ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Venn diagram Needs ggVennDiagram Input: infile: The input file for data If envs.intype is raw, it should be a data frame with row names as categories and only column as elements separated by comma ( , ) If it is computed , it should be a data frame with row names the elements and columns the categories. The data should be binary indicator ( 0, 1 ) indicating whether the elements are present in the categories. Output: outfile: The output figure file Envs: inopts: The options for read.table() to read in.infile intype: raw or computed . See in.infile devpars: The parameters for png() args: Additional arguments for ggVennDiagram() ggs: Additional ggplot expression to adjust the plot Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.plot . Heatmap ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot heatmaps using ComplexHeatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples >>> pipen run plot Heatmap >>> -- in . infile data . txt >>> -- in . annofiles anno . txt >>> -- envs . args . row_names_gp 'r:fontsize5' >>> -- envs . args . column_names_gp 'r:fontsize5' >>> -- envs . args . clustering_distance_rows pearson >>> -- envs . args . clustering_distance_columns pearson >>> -- envs . args . show_row_names false >>> -- envs . args . row_split 3 >>> -- args . devpars . width 5000 >>> -- args . devpars . height 5000 >>> -- args . draw . merge_legends >>> -- envs . args . heatmap_legend_param . title AUC >>> -- envs . args . row_dend_reorder >>> -- envs . args . column_dend_reorder >>> -- envs . args . top_annotation >>> 'r:HeatmapAnnotation( >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) >>> )' >>> -- envs . args . right_annotation >>> 'r:rowAnnotation( >>> AUC = anno_boxplot(as.matrix(data), outline = F) >>> )' >>> -- args . globals >>> 'fontsize8 = gpar(fontsize = 12); >>> fontsize5 = gpar(fontsize = 8); >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' >>> -- args . seed 8525 Input: infile: The data matrix file annofiles: The files for annotation data Output: outfile: The heatmap plot outdir: Other data of the heatmap Including RDS file of the heatmap, row clusters and col clusters. Envs: inopts: Options for read.table() to read in.infile anopts: Options for read.table() to read in.annofiles draw: Options for ComplexHeatmap::draw() args: Arguments for ComplexHeatmap::Heatmap() devpars: The parameters for device. seed: The seed globals: Some globals for the expression in args to be evaluated Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.plot"},{"location":"api/biopipen.namespaces.plot/#biopipennamespacesplot","text":"</> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> class","title":"biopipen.namespaces.plot"},{"location":"api/biopipen.namespaces.plot/#biopipennamespacesplotvenndiagram","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Venn diagram Needs ggVennDiagram Input: infile: The input file for data If envs.intype is raw, it should be a data frame with row names as categories and only column as elements separated by comma ( , ) If it is computed , it should be a data frame with row names the elements and columns the categories. The data should be binary indicator ( 0, 1 ) indicating whether the elements are present in the categories. Output: outfile: The output figure file Envs: inopts: The options for read.table() to read in.infile intype: raw or computed . See in.infile devpars: The parameters for png() args: Additional arguments for ggVennDiagram() ggs: Additional ggplot expression to adjust the plot Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.plot.VennDiagram"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.plot/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.plot/#biopipennamespacesplotheatmap","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot heatmaps using ComplexHeatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples >>> pipen run plot Heatmap >>> -- in . infile data . txt >>> -- in . annofiles anno . txt >>> -- envs . args . row_names_gp 'r:fontsize5' >>> -- envs . args . column_names_gp 'r:fontsize5' >>> -- envs . args . clustering_distance_rows pearson >>> -- envs . args . clustering_distance_columns pearson >>> -- envs . args . show_row_names false >>> -- envs . args . row_split 3 >>> -- args . devpars . width 5000 >>> -- args . devpars . height 5000 >>> -- args . draw . merge_legends >>> -- envs . args . heatmap_legend_param . title AUC >>> -- envs . args . row_dend_reorder >>> -- envs . args . column_dend_reorder >>> -- envs . args . top_annotation >>> 'r:HeatmapAnnotation( >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) >>> )' >>> -- envs . args . right_annotation >>> 'r:rowAnnotation( >>> AUC = anno_boxplot(as.matrix(data), outline = F) >>> )' >>> -- args . globals >>> 'fontsize8 = gpar(fontsize = 12); >>> fontsize5 = gpar(fontsize = 8); >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' >>> -- args . seed 8525 Input: infile: The data matrix file annofiles: The files for annotation data Output: outfile: The heatmap plot outdir: Other data of the heatmap Including RDS file of the heatmap, row clusters and col clusters. Envs: inopts: Options for read.table() to read in.infile anopts: Options for read.table() to read in.annofiles draw: Options for ComplexHeatmap::draw() args: Arguments for ComplexHeatmap::Heatmap() devpars: The parameters for device. seed: The seed globals: Some globals for the expression in args to be evaluated Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.plot.Heatmap"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.plot/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.plot/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.rnaseq/","text":"module biopipen.namespaces . rnaseq </> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> class biopipen.namespaces.rnaseq . UnitConversion ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert expression value units back and forth Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.rnaseq"},{"location":"api/biopipen.namespaces.rnaseq/#biopipennamespacesrnaseq","text":"</> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> class","title":"biopipen.namespaces.rnaseq"},{"location":"api/biopipen.namespaces.rnaseq/#biopipennamespacesrnasequnitconversion","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert expression value units back and forth Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.rnaseq.UnitConversion"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.rnaseq/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/","text":"module biopipen.namespaces . scrna </> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Seurat - Loading and preparing data </> SeuratClustering ( Proc ) \u2014 Seurat - Determine the clusters </> GeneExpressionInvestigation ( Proc ) \u2014 Investigation of expressions of genes of interest </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> class biopipen.namespaces.scrna . SeuratLoading ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - RNADir to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output: rdsfile: The RDS file with a list of Seurat object Envs: qc: The QC filter for each sample. This will be passed to subset(obj, subset=<qc>) . For example nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5 Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . SeuratPreparing ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading and preparing data What will be done ? https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1) 1. All samples with be integrated as a single seurat object 2. QC 3. Normalization 4. Feature selection 5. Scaling 6. Linear dimensional reduction Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - RNADir to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output: rdsfile: The RDS file with the Seurat object Note that the cell ids are preficed with sample names Envs: ncores: Number of cores to use Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . SeuratClustering ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Determine the clusters Input: srtobj: The seurat object loaded by SeuratPreparing Output: rdsfile: The seurat object with cluster information groupfile: A groupfile with cells for future analysis Envs: FindClusters: Arguments to FindClusters() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . GeneExpressionInvestigation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Investigation of expressions of genes of interest Input: srtobj: The seurat object loaded by SeuratPreparing groupfile: The group of cells with the first column the groups and rest the cells in each sample. Or the subset conditions using metadata of srtobj See envs.group_subset genefiles: The genes to show their expressions in the plots configfile: The configuration file (toml). See envs If not provided, use envs Output: outdir: The output directory with the plots Envs: group_subset: Is the in.groupfile subset conditions using metadata Or the groupfile as described. name: The name to name the job. Otherwise the stem of groupfile will be used target: Which sample to pull expression from could be multiple gopts: Options for read.table() to read the genefiles plots: Plots to generate for this case boxplot : - use : Which gene file to use (1-based) - ncol : Split the plot to how many columns? - res , height and width the parameters for png() heatmap : - use : Which gene file to use (1-based) - res , height and width the parameters for png() - other arguments for ComplexHeatmap::Heatmap() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . DimPlots ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Dimensional reduction plots Input: srtobj: The seruat object in RDS format configfile: A toml configuration file with \"cases\" If this is given, envs.cases will be overriden name: The name of the job, used in report Output: outdir: The output directory Envs: cases: The cases for the dim plots Keys are the names and values are the arguments to Seurat::Dimplots Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . MarkersFinder ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between different groups of cells Input: srtobj: The seurat object loaded by SeuratLoading groupfile: The group of cells with first column the groups and rest the cells in each sample. casefile: The config file in TOML that looks like >>> [ case1 ] >>> \"ident.1\" = \"ident.1\" >>> \"ident.2\" = \"ident.2\" >>> # other arguments for Seruat::FindMarkers() : The name of the jobs , mosted used in report Output: outdir: The output directory for the markers Envs: ncores: Number of cores to use to parallelize the groups cases: The cases to find markers for. Values would be the arguments for FindMarkers() If \"ALL\" or \"ALL\" in the keys, the process will run for all groups in the groupfile. The other keys will be arguments to FindMarkers When ident.2 is not given and there is only one group or more than two groups in groupfile, the rest cells in the object will be used as the control if it is ident , will igore the groupfile and find markers for all idents. dbs: The dbs to do enrichment analysis for significant markers See below for all librarys https://maayanlab.cloud/Enrichr/#libraries Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . SCImpute ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Impute the dropout values in scRNA-seq data. Input: infile: The input file for imputation Either a SeuratObject or a matrix of count/TPM groupfile: The file to subset the matrix or label the cells Could be an output from ImmunarchFilter Output: outfile: The output matrix Envs: infmt: The input format. Either seurat or `matrix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.scrna . SeuratFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Filtering cells from a seurat object Input: srtobj: The seurat object in RDS filterfile: The file with the filtering information Either a group file (rows cases for filtering, columns are samples or ALL for all cells with prefices), or config under subsetting section in TOML with keys and value that will be passed to subset(...,subset = ...) Output: out: The filtered seurat object in RDS if envs.multicase is False, otherwise the directory with the filtered seurat objects Envs: filterfmt: auto , subset or grouping . If subset then in.filterfile will be config in TOML, otherwise if grouping , it is a groupfile. See in.filterfile . If auto , test if there is = in the file. If so, it's subset otherwise grouping invert: Invert the selection? multicase: If True, multiple seurat objects will be generated. For envs.filterfmt == \"subset\" , each key-value pair will be a case, otherwise, each row of in.filterfile will be a case. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.scrna"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrna","text":"</> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Seurat - Loading and preparing data </> SeuratClustering ( Proc ) \u2014 Seurat - Determine the clusters </> GeneExpressionInvestigation ( Proc ) \u2014 Investigation of expressions of genes of interest </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> class","title":"biopipen.namespaces.scrna"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnaseuratloading","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - RNADir to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output: rdsfile: The RDS file with a list of Seurat object Envs: qc: The QC filter for each sample. This will be passed to subset(obj, subset=<qc>) . For example nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5 Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.SeuratLoading"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnaseuratpreparing","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading and preparing data What will be done ? https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1) 1. All samples with be integrated as a single seurat object 2. QC 3. Normalization 4. Feature selection 5. Scaling 6. Linear dimensional reduction Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - RNADir to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output: rdsfile: The RDS file with the Seurat object Note that the cell ids are preficed with sample names Envs: ncores: Number of cores to use Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.SeuratPreparing"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnaseuratclustering","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Determine the clusters Input: srtobj: The seurat object loaded by SeuratPreparing Output: rdsfile: The seurat object with cluster information groupfile: A groupfile with cells for future analysis Envs: FindClusters: Arguments to FindClusters() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.SeuratClustering"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnageneexpressioninvestigation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Investigation of expressions of genes of interest Input: srtobj: The seurat object loaded by SeuratPreparing groupfile: The group of cells with the first column the groups and rest the cells in each sample. Or the subset conditions using metadata of srtobj See envs.group_subset genefiles: The genes to show their expressions in the plots configfile: The configuration file (toml). See envs If not provided, use envs Output: outdir: The output directory with the plots Envs: group_subset: Is the in.groupfile subset conditions using metadata Or the groupfile as described. name: The name to name the job. Otherwise the stem of groupfile will be used target: Which sample to pull expression from could be multiple gopts: Options for read.table() to read the genefiles plots: Plots to generate for this case boxplot : - use : Which gene file to use (1-based) - ncol : Split the plot to how many columns? - res , height and width the parameters for png() heatmap : - use : Which gene file to use (1-based) - res , height and width the parameters for png() - other arguments for ComplexHeatmap::Heatmap() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.GeneExpressionInvestigation"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnadimplots","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Dimensional reduction plots Input: srtobj: The seruat object in RDS format configfile: A toml configuration file with \"cases\" If this is given, envs.cases will be overriden name: The name of the job, used in report Output: outdir: The output directory Envs: cases: The cases for the dim plots Keys are the names and values are the arguments to Seurat::Dimplots Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.DimPlots"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnamarkersfinder","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between different groups of cells Input: srtobj: The seurat object loaded by SeuratLoading groupfile: The group of cells with first column the groups and rest the cells in each sample. casefile: The config file in TOML that looks like >>> [ case1 ] >>> \"ident.1\" = \"ident.1\" >>> \"ident.2\" = \"ident.2\" >>> # other arguments for Seruat::FindMarkers() : The name of the jobs , mosted used in report Output: outdir: The output directory for the markers Envs: ncores: Number of cores to use to parallelize the groups cases: The cases to find markers for. Values would be the arguments for FindMarkers() If \"ALL\" or \"ALL\" in the keys, the process will run for all groups in the groupfile. The other keys will be arguments to FindMarkers When ident.2 is not given and there is only one group or more than two groups in groupfile, the rest cells in the object will be used as the control if it is ident , will igore the groupfile and find markers for all idents. dbs: The dbs to do enrichment analysis for significant markers See below for all librarys https://maayanlab.cloud/Enrichr/#libraries Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.MarkersFinder"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnascimpute","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Impute the dropout values in scRNA-seq data. Input: infile: The input file for imputation Either a SeuratObject or a matrix of count/TPM groupfile: The file to subset the matrix or label the cells Could be an output from ImmunarchFilter Output: outfile: The output matrix Envs: infmt: The input format. Either seurat or `matrix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.SCImpute"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna/#biopipennamespacesscrnaseuratfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Filtering cells from a seurat object Input: srtobj: The seurat object in RDS filterfile: The file with the filtering information Either a group file (rows cases for filtering, columns are samples or ALL for all cells with prefices), or config under subsetting section in TOML with keys and value that will be passed to subset(...,subset = ...) Output: out: The filtered seurat object in RDS if envs.multicase is False, otherwise the directory with the filtered seurat objects Envs: filterfmt: auto , subset or grouping . If subset then in.filterfile will be config in TOML, otherwise if grouping , it is a groupfile. See in.filterfile . If auto , test if there is = in the file. If so, it's subset otherwise grouping invert: Invert the selection? multicase: If True, multiple seurat objects will be generated. For envs.filterfmt == \"subset\" , each key-value pair will be a case, otherwise, each row of in.filterfile will be a case. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.scrna.SeuratFilter"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.scrna/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.scrna/#pipenprocprocrun_7","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.scrna_metabolic/","text":"module biopipen.namespaces . scrna_metabolic </> Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape If you have clustering done somewhere else, you could use replace_clustering() See docs here https://pwwang.github.io/biopipen/pipelines/scrna_metabolic Reference: [1] Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Start Process: MetabolicInputs Functions build_processes ( options ) \u2014 Build processes for metabolic landscape analysis pipeline </> metabolic_landscape ( ) (Pipen) \u2014 Build a pipeline for pipen run to run </> function biopipen.namespaces.scrna_metabolic . build_processes ( options=None ) </> Build processes for metabolic landscape analysis pipeline function biopipen.namespaces.scrna_metabolic . metabolic_landscape ( ) \u2192 Pipen </> Build a pipeline for pipen run to run","title":"biopipen.namespaces.scrna_metabolic"},{"location":"api/biopipen.namespaces.scrna_metabolic/#biopipennamespacesscrna_metabolic","text":"</> Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape If you have clustering done somewhere else, you could use replace_clustering() See docs here https://pwwang.github.io/biopipen/pipelines/scrna_metabolic Reference: [1] Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Start Process: MetabolicInputs Functions build_processes ( options ) \u2014 Build processes for metabolic landscape analysis pipeline </> metabolic_landscape ( ) (Pipen) \u2014 Build a pipeline for pipen run to run </> function","title":"biopipen.namespaces.scrna_metabolic"},{"location":"api/biopipen.namespaces.scrna_metabolic/#biopipennamespacesscrna_metabolicbuild_processes","text":"</> Build processes for metabolic landscape analysis pipeline function","title":"biopipen.namespaces.scrna_metabolic.build_processes"},{"location":"api/biopipen.namespaces.scrna_metabolic/#biopipennamespacesscrna_metabolicmetabolic_landscape","text":"</> Build a pipeline for pipen run to run","title":"biopipen.namespaces.scrna_metabolic.metabolic_landscape"},{"location":"api/biopipen.namespaces.tcr/","text":"module biopipen.namespaces . tcr </> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> ImmunarchBasic ( Proc ) \u2014 Immunarch - Basic statistics and clonality </> ImmunarchAdvanced ( Proc ) \u2014 Immunarch - Advanced analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> class biopipen.namespaces.tcr . ImmunarchLoading ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immuarch - Loading data Build based on immunarch 0.6.7 See https://immunarch.com/articles/v2_data.html for supported data formats Currently only 10x data format is supported Library dplyr is also required to manipulate the meta data. Input: metafile: The meta data of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - TCRDir to assign the path of the data to the samples, and this column will be excluded as metadata. Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like filtered_contig_annotations.csv , which doesn't have any name information. Output: rdsfile: The RDS file with the data and metadata Envs: tmpdir: The temporary directory to link all data files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . ImmunarchFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Filter data See https://immunarch.com/articles/web_only/repFilter_v3.html Input: immdata: The data loaded by immunarch::repLoad() filterfile: A config file in TOML. A dict of configurations with keys as the names of the group and values dicts with following keys. See envs.filters Output: outfile: The filtered immdata groupfile: Also a group file with first column the groups and other columns the cell barcodes in the samples Envs: merge: Merge the cells from the samples, instead of list cells for different samples. The cell ids will be preficed with the sample name, connected with _ . The column name will be ALL instead. clonotype: Use clonotype (CDR3.aa) as the group. The name from envs.filters will be ignored filters: The filters to filter the data You can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by immunarch::repFilter() . There is one more method by.count supported to filter the count matrix. For by.meta , by.repertoire , by.rep , by.clonotype or by.col the values will be passed to .query of repFilter() . You can also use the helper functions provided by immunarch , including morethan , lessthan , include , exclude and interval . If these functions are not used, include(value) will be used by default. For by.count , the value of filter will be passed to dplyr::filter() to filter the count matrix. You can also specify ORDER to define the filtration order, which defaults to 0, higher ORDER gets later executed. For example: >>> {{ >>> \"Top20BM_Post\" : {{ >>> \"by.meta\" : {{ \"Source\" : \"BM\" , \"Status\" : \"Post\" }}, >>> \"by.count\" : {{ >>> \"ORDER\" : 1 , \"filter\" : \"TOTAL %i n% TOTAL[1:20]\" >>> }} >>> }} >>> }} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . ImmunarchBasic ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Basic statistics and clonality See https://immunarch.com/articles/web_only/v3_basic_analysis.html Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: volume_by: Groupings to show clonotype volume (sizes) Multiple groups supported, for example: volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}} Or label the groups: volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\", \"Sex\"]}} If a list or a single variable is given, it will be changed into {{\"Status\": \"Status\"}} len_by: Groupings to show CDR3 length of both aa and nt count_by: Groupings to show clonotype counts per sample top_clone_marks: .head arguments of repClonoality() top_clone_by: Groupings when visualize top clones rare_clone_marks: .bound arguments of repClonoality() rare_clone_by: Groupings when visualize rare clones hom_clone_marks: .clone.types arguments of repClonoality() hom_clone_by: Groupings when visualize homeo clones overlap_methods: The methods used for repOverlap() , each will generate a heatmap. overlap_redim: Plot the samples with these dimension reduction methods Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . ImmunarchAdvanced ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Advanced analysis Including gene usage, diversity estimation, tracking clonotype changes and kmer/motif analysis Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: gu_by: Groupings to show gene usages Multiple groups supported, for example: volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}} Or label the groups: volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\",\"Sex\"]}} If a list or a single variable is given, it will be changed into {{\"Status\": \"Status\"}} gu_top: How many top (ranked by total usage across samples) genes to show in the plots gua_methods: controls how the data is going to be preprocessed and analysed. One of js, cor, cosine, pca, mds, and tsne spect: .quant and .col for spectratype() for each sample div_methods: Methods to calculate diversities div_by: Groupings to show sample diversities raref_by: Groupings to show rarefactions tracking_target: and tracking_samples: The target and samples to tracking You can do multiple trackings. To do that, you need to specify a key for each tracking. It will use the target and samples under the same key. If samples from tracking_samples cannot be found, all samples will be used Other than the target supported by immunarch, you can also specify top shared clones. For example: tracking_target = {{ \"top_4\": {{\"TOP\": 4}} }} kmers: Arguments for kmer analysis. Keys are the K of mers. Values are parameters: - head specifies # of the most abundant kmers to visualise. - position : positions of bars: stack , dodge and fill - log : log-transformation of y-axis - motif : Method for motif analysis There can be multiple head s and motif s. If you do want multiple parameter sets for the same K, You can use a float number as the K. For example: 5.1 for K 5 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . CloneResidency ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Identification of clone residency Typically, where the clones are located for the sample patient. Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: subject: The key of subject in metadata. The clone residency will be examined for this subject/patient group: The key of group in metadata. This usually marks the samples that you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. order: The order of the values in group . Early-ordered group will be used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the scatter plots will be drawn for pairs: B ~ A, C ~ B. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . Immunarch2VDJtools ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert immuarch format into VDJtools input formats Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory containing the vdjtools input for each sample Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . VJUsage ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. Input: infile: The input file, in vdjtools input format Output: outfile: The V-J usage plot Envs: vdjtools: The path to vdjtools vdjtools_patch: A patch for vdjtools Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . Attach2Seurat ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Attach the clonal information to a Seurat object as metadata Input: immfile: The immunarch object in RDS sobjfile: The Seurat object file in RDS Output: outfile: The Seurat object with the clonal information as metadata Envs: prefix: The prefix to the barcodes. You can use placeholder like {{Sample}}_ to use the meta data from the immunarch object metacols: Which meta columns to attach Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.tcr . TCRClustering ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Cluster the TCR clones by their CDR3 sequences With GIANA https://github.com/s175573/GIANA Zhang, Hongyi, Xiaowei Zhan, and Bo Li. \"GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation.\" Nature communications 12.1 (2021): 1-11. Or ClusTCR https://github.com/svalkiers/clusTCR Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, ClusTCR: a Python interface for rapid clustering of large sets of CDR3 sequences with unknown antigen specificity, Bioinformatics, 2021. Input: immfile: The immunarch object in RDS Output: immfile: The immnuarch object in RDS with TCR cluster information clusterfile: The cluster file. Columns are CDR3.aa, TCR_Cluster, V.name, Sample heatmap: The heatmap of the samples, in terms of their shared TCR Clusters Envs: tool: The tool used to do the clustering, either GIANA or ClusTCR For GIANA, using TRBV mutations is not supported on_raw: Whether to run clustering on raw seq or the seq read and processed by immunarch python: The path of python with GIANA 's dependencies installed or with clusTCR installed. Depending on the tool you choose. tmpdir: The temporary directory to store the GIANA sources giana_source: The URLs for the source code of GIANA args: The arguments for the clustering tool For GIANA, they will be passed to python GIAna.py For ClusTCR, they will be passed to clustcr.Clustering(...) heatmap_meta: The metadata to show in the heatmap for each sample Current only support categorical/character metadata numbers_on_heatmap: Whether to show the numbers on the heatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.tcr"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcr","text":"</> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> ImmunarchBasic ( Proc ) \u2014 Immunarch - Basic statistics and clonality </> ImmunarchAdvanced ( Proc ) \u2014 Immunarch - Advanced analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> class","title":"biopipen.namespaces.tcr"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrimmunarchloading","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immuarch - Loading data Build based on immunarch 0.6.7 See https://immunarch.com/articles/v2_data.html for supported data formats Currently only 10x data format is supported Library dplyr is also required to manipulate the meta data. Input: metafile: The meta data of the samples A tab-delimited file Two columns are required: - Sample to specify the sample names. - TCRDir to assign the path of the data to the samples, and this column will be excluded as metadata. Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like filtered_contig_annotations.csv , which doesn't have any name information. Output: rdsfile: The RDS file with the data and metadata Envs: tmpdir: The temporary directory to link all data files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.ImmunarchLoading"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrimmunarchfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Filter data See https://immunarch.com/articles/web_only/repFilter_v3.html Input: immdata: The data loaded by immunarch::repLoad() filterfile: A config file in TOML. A dict of configurations with keys as the names of the group and values dicts with following keys. See envs.filters Output: outfile: The filtered immdata groupfile: Also a group file with first column the groups and other columns the cell barcodes in the samples Envs: merge: Merge the cells from the samples, instead of list cells for different samples. The cell ids will be preficed with the sample name, connected with _ . The column name will be ALL instead. clonotype: Use clonotype (CDR3.aa) as the group. The name from envs.filters will be ignored filters: The filters to filter the data You can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by immunarch::repFilter() . There is one more method by.count supported to filter the count matrix. For by.meta , by.repertoire , by.rep , by.clonotype or by.col the values will be passed to .query of repFilter() . You can also use the helper functions provided by immunarch , including morethan , lessthan , include , exclude and interval . If these functions are not used, include(value) will be used by default. For by.count , the value of filter will be passed to dplyr::filter() to filter the count matrix. You can also specify ORDER to define the filtration order, which defaults to 0, higher ORDER gets later executed. For example: >>> {{ >>> \"Top20BM_Post\" : {{ >>> \"by.meta\" : {{ \"Source\" : \"BM\" , \"Status\" : \"Post\" }}, >>> \"by.count\" : {{ >>> \"ORDER\" : 1 , \"filter\" : \"TOTAL %i n% TOTAL[1:20]\" >>> }} >>> }} >>> }} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.ImmunarchFilter"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrimmunarchbasic","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Basic statistics and clonality See https://immunarch.com/articles/web_only/v3_basic_analysis.html Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: volume_by: Groupings to show clonotype volume (sizes) Multiple groups supported, for example: volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}} Or label the groups: volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\", \"Sex\"]}} If a list or a single variable is given, it will be changed into {{\"Status\": \"Status\"}} len_by: Groupings to show CDR3 length of both aa and nt count_by: Groupings to show clonotype counts per sample top_clone_marks: .head arguments of repClonoality() top_clone_by: Groupings when visualize top clones rare_clone_marks: .bound arguments of repClonoality() rare_clone_by: Groupings when visualize rare clones hom_clone_marks: .clone.types arguments of repClonoality() hom_clone_by: Groupings when visualize homeo clones overlap_methods: The methods used for repOverlap() , each will generate a heatmap. overlap_redim: Plot the samples with these dimension reduction methods Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.ImmunarchBasic"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrimmunarchadvanced","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Advanced analysis Including gene usage, diversity estimation, tracking clonotype changes and kmer/motif analysis Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: gu_by: Groupings to show gene usages Multiple groups supported, for example: volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}} Or label the groups: volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\",\"Sex\"]}} If a list or a single variable is given, it will be changed into {{\"Status\": \"Status\"}} gu_top: How many top (ranked by total usage across samples) genes to show in the plots gua_methods: controls how the data is going to be preprocessed and analysed. One of js, cor, cosine, pca, mds, and tsne spect: .quant and .col for spectratype() for each sample div_methods: Methods to calculate diversities div_by: Groupings to show sample diversities raref_by: Groupings to show rarefactions tracking_target: and tracking_samples: The target and samples to tracking You can do multiple trackings. To do that, you need to specify a key for each tracking. It will use the target and samples under the same key. If samples from tracking_samples cannot be found, all samples will be used Other than the target supported by immunarch, you can also specify top shared clones. For example: tracking_target = {{ \"top_4\": {{\"TOP\": 4}} }} kmers: Arguments for kmer analysis. Keys are the K of mers. Values are parameters: - head specifies # of the most abundant kmers to visualise. - position : positions of bars: stack , dodge and fill - log : log-transformation of y-axis - motif : Method for motif analysis There can be multiple head s and motif s. If you do want multiple parameter sets for the same K, You can use a float number as the K. For example: 5.1 for K 5 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.ImmunarchAdvanced"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrcloneresidency","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Identification of clone residency Typically, where the clones are located for the sample patient. Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory Envs: subject: The key of subject in metadata. The clone residency will be examined for this subject/patient group: The key of group in metadata. This usually marks the samples that you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. order: The order of the values in group . Early-ordered group will be used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the scatter plots will be drawn for pairs: B ~ A, C ~ B. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.CloneResidency"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrimmunarch2vdjtools","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert immuarch format into VDJtools input formats Input: immdata: The data loaded by immunarch::repLoad() Output: outdir: The output directory containing the vdjtools input for each sample Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.Immunarch2VDJtools"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrvjusage","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions. Input: infile: The input file, in vdjtools input format Output: outfile: The V-J usage plot Envs: vdjtools: The path to vdjtools vdjtools_patch: A patch for vdjtools Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.VJUsage"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrattach2seurat","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Attach the clonal information to a Seurat object as metadata Input: immfile: The immunarch object in RDS sobjfile: The Seurat object file in RDS Output: outfile: The Seurat object with the clonal information as metadata Envs: prefix: The prefix to the barcodes. You can use placeholder like {{Sample}}_ to use the meta data from the immunarch object metacols: Which meta columns to attach Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.Attach2Seurat"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.tcr/#biopipennamespacestcrtcrclustering","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Cluster the TCR clones by their CDR3 sequences With GIANA https://github.com/s175573/GIANA Zhang, Hongyi, Xiaowei Zhan, and Bo Li. \"GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation.\" Nature communications 12.1 (2021): 1-11. Or ClusTCR https://github.com/svalkiers/clusTCR Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, ClusTCR: a Python interface for rapid clustering of large sets of CDR3 sequences with unknown antigen specificity, Bioinformatics, 2021. Input: immfile: The immunarch object in RDS Output: immfile: The immnuarch object in RDS with TCR cluster information clusterfile: The cluster file. Columns are CDR3.aa, TCR_Cluster, V.name, Sample heatmap: The heatmap of the samples, in terms of their shared TCR Clusters Envs: tool: The tool used to do the clustering, either GIANA or ClusTCR For GIANA, using TRBV mutations is not supported on_raw: Whether to run clustering on raw seq or the seq read and processed by immunarch python: The path of python with GIANA 's dependencies installed or with clusTCR installed. Depending on the tool you choose. tmpdir: The temporary directory to store the GIANA sources giana_source: The URLs for the source code of GIANA args: The arguments for the clustering tool For GIANA, they will be passed to python GIAna.py For ClusTCR, they will be passed to clustcr.Clustering(...) heatmap_meta: The metadata to show in the heatmap for each sample Current only support categorical/character metadata numbers_on_heatmap: Whether to show the numbers on the heatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.tcr.TCRClustering"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.tcr/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.tcr/#pipenprocprocrun_8","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.vcf/","text":"module biopipen.namespaces . vcf </> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> class biopipen.namespaces.vcf . VcfLiftOver ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a VCF file using GATK Input: invcf: The input VCF file Output: outvcf: The output VCF file Envs: gatk: The path to gatk4, which should be installed via conda chain: The map chain file for liftover tmpdir: Directory for temporary storage of working files args: Other CLI arguments for gatk LiftoverVcf Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.vcf . VcfFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter records in vcf file Input: invcf: The input vcf file, could be bgzipped. Output: outfile: The filtered vcf file. If in.invcf is bgzipped, then this will be bgzipped. Envs: filters: A dict of filters with keys the filter names. >>> # Typically >>> lambda variant : < expression > Things to notice 1. Filters should return ` False ` to get variant filtered out 2. See https : // brentp . github . io / cyvcf2 / docstrings . html #cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters ` { \"QUAL\" : 30 } ` 5. List of builtin filters . Specify them like : ` { \"FILTER\" : params } ` ` SNPONLY ` : keeps only SNPs ( ` { \"SNPONLY\" : False } ` to filter SNPs out ) ` QUAL ` : keeps variants with QUAL >= param ( ` { \"QUAL\" : ( 30 , False )} ` ) to keep only variants with QUAL < 30 er_descs : Descriptions for the filters . Will be saved to the header of the output vcf file er : Some helper code for the filters : Keep the variants not passing the filters ? Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.vcf . VcfIndex ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Index VCF files. If they are already index, use the index files Input: infile: The input VCF file Output: outfile: The output VCF file (bgzipped) outidx: The index file of the output VCF file Envs: tabix: Path to tabix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.vcf . VcfDownSample ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Down-sample VCF files to keep only a subset of variants in there Input: infile: The input VCF file Output: outfile: The output VCF file with subet variants Gzipped if in.infile is gzipped Envs: n: Fraction/Number of variants to keep If n > 1 , it is the number. If n <= 1 , it is the fraction. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.vcf"},{"location":"api/biopipen.namespaces.vcf/#biopipennamespacesvcf","text":"</> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> class","title":"biopipen.namespaces.vcf"},{"location":"api/biopipen.namespaces.vcf/#biopipennamespacesvcfvcfliftover","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a VCF file using GATK Input: invcf: The input VCF file Output: outvcf: The output VCF file Envs: gatk: The path to gatk4, which should be installed via conda chain: The map chain file for liftover tmpdir: Directory for temporary storage of working files args: Other CLI arguments for gatk LiftoverVcf Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.vcf.VcfLiftOver"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.vcf/#biopipennamespacesvcfvcffilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter records in vcf file Input: invcf: The input vcf file, could be bgzipped. Output: outfile: The filtered vcf file. If in.invcf is bgzipped, then this will be bgzipped. Envs: filters: A dict of filters with keys the filter names. >>> # Typically >>> lambda variant : < expression > Things to notice 1. Filters should return ` False ` to get variant filtered out 2. See https : // brentp . github . io / cyvcf2 / docstrings . html #cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters ` { \"QUAL\" : 30 } ` 5. List of builtin filters . Specify them like : ` { \"FILTER\" : params } ` ` SNPONLY ` : keeps only SNPs ( ` { \"SNPONLY\" : False } ` to filter SNPs out ) ` QUAL ` : keeps variants with QUAL >= param ( ` { \"QUAL\" : ( 30 , False )} ` ) to keep only variants with QUAL < 30 er_descs : Descriptions for the filters . Will be saved to the header of the output vcf file er : Some helper code for the filters : Keep the variants not passing the filters ? Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.vcf.VcfFilter"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.vcf/#biopipennamespacesvcfvcfindex","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Index VCF files. If they are already index, use the index files Input: infile: The input VCF file Output: outfile: The output VCF file (bgzipped) outidx: The index file of the output VCF file Envs: tabix: Path to tabix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.vcf.VcfIndex"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.vcf/#biopipennamespacesvcfvcfdownsample","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Down-sample VCF files to keep only a subset of variants in there Input: infile: The input VCF file Output: outfile: The output VCF file with subet variants Gzipped if in.infile is gzipped Envs: n: Fraction/Number of variants to keep If n > 1 , it is the number. If n <= 1 , it is the fraction. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.vcf.VcfDownSample"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.vcf/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.vcf/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.web/","text":"module biopipen.namespaces . web </> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class biopipen.namespaces.web . Download ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs Input: url: The URL to download data from Output: outfile: The file downloaded Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.namespaces.web . DownloadList ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.namespaces.web"},{"location":"api/biopipen.namespaces.web/#biopipennamespacesweb","text":"</> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class","title":"biopipen.namespaces.web"},{"location":"api/biopipen.namespaces.web/#biopipennamespaceswebdownload","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs Input: url: The URL to download data from Output: outfile: The file downloaded Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.web.Download"},{"location":"api/biopipen.namespaces.web/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.web/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.web/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.web/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.web/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.web/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.namespaces.web/#biopipennamespaceswebdownloadlist","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (type of Proc) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.namespaces.web.DownloadList"},{"location":"api/biopipen.namespaces.web/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (any) \u2014 and **kwds (any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.namespaces.web/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (type of Proc) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (optional) \u2014 The new plugin options, unspecified items will beinherited. requires (sequence of type of Proc, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (type of Proc) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.namespaces.web/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.namespaces.web/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.namespaces.web/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.namespaces.web/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/source/biopipen.core.config/","text":"SOURCE CODE biopipen.core. config DOCS \"\"\"Provides the envs from configuration files\"\"\" import sys from typing import Any from pathlib import Path from tempfile import gettempdir from diot import Diot from simpleconf import Config from .defaults import BIOPIPEN_DIR DEFAULT_CONFIG_FILE = BIOPIPEN_DIR / \"core\" / \"config.toml\" USER_CONFIG_FILE = Path ( \"~\" ) . expanduser () / \".biopipen.toml\" PROJ_CONFIG_FILE = Path ( \".\" ) / \".biopipen.toml\" class ConfigItems ( Diot ): DOCS \"\"\"Provides the envs from configuration files and defaults the non-existing values to None.\"\"\" def __getattr__ ( self , name : str ) -> Any : try : return super () . __getattr__ ( name ) except ( KeyError , AttributeError ): return None def __getitem__ ( self , name : str ) -> Any : DOCS try : return super () . __getitem__ ( name ) except ( KeyError , AttributeError ): return None config_profiles = [ { \"path\" : { \"tmpdir\" : gettempdir ()}}, DEFAULT_CONFIG_FILE , USER_CONFIG_FILE , PROJ_CONFIG_FILE , ] # scan sys.argv to see if --config <config file> passed in if \"+config\" in sys . argv : cindex = sys . argv . index ( \"+config\" ) config_profiles . append ( sys . argv [ cindex + 1 ]) config = ConfigItems ( Config . load ( * config_profiles , ignore_nonexist = True ))","title":"biopipen.core.config"},{"location":"api/source/biopipen.core.defaults/","text":"SOURCE CODE biopipen.core. defaults DOCS \"\"\"Provide default settgins\"\"\" from pathlib import Path BIOPIPEN_DIR = Path ( __file__ ) . parent . parent . resolve () REPORT_DIR = BIOPIPEN_DIR / \"reports\" SCRIPT_DIR = BIOPIPEN_DIR / \"scripts\"","title":"biopipen.core.defaults"},{"location":"api/source/biopipen.core/","text":"SOURCE CODE biopipen. core DOCS","title":"biopipen.core"},{"location":"api/source/biopipen.core.proc/","text":"SOURCE CODE biopipen.core. proc DOCS \"\"\"Provides a base class for the processes to subclass\"\"\" from liquid.defaults import SEARCH_PATHS from pipen import Proc as PipenProc from .filters import filtermanager from .defaults import BIOPIPEN_DIR , REPORT_DIR class Proc ( PipenProc ): DOCS \"\"\"Base class for all processes in biopipen to subclass\"\"\" template_opts = { \"globals\" : { \"biopipen_dir\" : str ( BIOPIPEN_DIR ), }, \"filters\" : filtermanager . filters . copy (), \"search_paths\" : SEARCH_PATHS + [ str ( REPORT_DIR )] }","title":"biopipen.core.proc"},{"location":"api/source/biopipen.namespaces.bam/","text":"SOURCE CODE biopipen.namespaces. bam DOCS \"\"\"Tools to process sam/bam/cram files\"\"\" from ..core.proc import Proc from ..core.config import config class CNVpytor ( Proc ): DOCS \"\"\"Detect CNV using CNVpytor Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: cnvpytor: Path to cnvpytor ncores: Number of cores to use (`-j` for cnvpytor) cases: Cases to run cnvpytor. A dictionary with keys as case names and values is also a dict - `chrom`: The chromosomes to run on - `binsizes`: The binsizes - `snp`: How to read snp data - `mask_snps`: Whether mask 1000 Genome snps - `baf_nomask`: Do not use P mask in BAF histograms \"\"\" input = \"bamfile:file, snpfile:file\" output = \"outdir:dir:{{in.bamfile | stem}}.cnvpytor\" lang = config . lang . python envs = { \"cnvpytor\" : config . exe . cnvpytor , \"ncores\" : config . misc . ncores , \"cases\" : { \"Basic\" : { \"chrom\" : [], \"binsizes\" : [ 10000 , 100000 ], # set False to disable snp data importing \"snp\" : { \"sample\" : \"\" , \"name1\" : [], \"ad\" : \"AD\" , \"gt\" : \"GT\" , \"noAD\" : False , }, \"mask_snps\" : True , \"baf_nomask\" : False , # other arguments for -rd } }, } script = \"file://../scripts/bam/CNVpytor.py\" plugin_opts = { \"report\" : \"file://../reports/bam/CNVpytor.svelte\" } class ControlFREEC ( Proc ): DOCS \"\"\"Detect CNVs using Control-FREEC Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: freec: Path to Control-FREEC executable ncores: Number of cores to use arggs: Other arguments for Control-FREEC \"\"\" input = \"bamfile:file, snpfile:file\" output = \"outdir:dir:{{in.bamfile | stem}}.freec\" lang = config . lang . python envs = { \"freec\" : config . exe . freec , \"ncores\" : config . misc . ncores , \"tabix\" : config . exe . tabix , \"bedtools\" : config . exe . bedtools , \"sambamba\" : config . exe . sambamba , \"samtools\" : config . exe . samtools , # The \"<ref>.fai\" file will be used as chrLenFile \"ref\" : config . ref . reffa , \"refdir\" : config . ref . refdir , \"rscript\" : config . lang . rscript , \"binsize\" : 50_000 , # shortcut for args.general.window \"args\" : { \"general\" : { \"ploidy\" : 2 , \"breakPointThreshold\" : 0.8 , }, \"sample\" : { \"mateOrientation\" : \"FR\" }, \"control\" : {}, \"BAF\" : {}, \"target\" : {}, } } script = \"file://../scripts/bam/ControlFREEC.py\" plugin_opts = { \"report\" : \"file://../reports/bam/ControlFREEC.svelte\" }","title":"biopipen.namespaces.bam"},{"location":"api/source/biopipen.namespaces.bcftools/","text":"SOURCE CODE biopipen.namespaces. bcftools DOCS \"\"\"handling VCF files using bcftools\"\"\" from ..core.proc import Proc from ..core.config import config class BcftoolsAnnotate ( Proc ): DOCS \"\"\"Add or remove annotations from VCF files Input: infile: The input VCF file annfile: The annotation file Output: outfile: The annotated VCF file Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile and annfile annfile: The annotation file. If `in.annfile` is provided, this is ignored ncores: Number of cores (`--nthread`) to use cols: Overwrite `-c/--columns` header: Headers to be added args: Other arguments for `bcftools annotate` \"\"\" input = \"infile:file, annfile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"annfile\" : \"\" , \"cols\" : [], \"header\" : [], \"args\" : {}, \"ncores\" : config . misc . ncores , } script = \"file://../scripts/bcftools/BcftoolsAnnotate.py\" class BcftoolsFilter ( Proc ): DOCS \"\"\"Apply fixed threshold filters to VCF files Input: infile: The input VCF file Output: outfile: The filtered VCF file. If the `in.infile` is gzipped, this is gzipped as well. Envs: bcftools: Path to bcftools ncores: Number of cores (`--nthread`) to use keep: Whether we should keep the filtered variants or not. args: Other arguments for `bcftools annotate` ncores: `nthread` tmpdir: Path to save the intermediate files Since the filters need to be applied one by one by bcftools includes: and excludes: include/exclude only sites for which EXPRESSION is true. - See: https://samtools.github.io/bcftools/bcftools.html#expressions - If provided, `envs.args.include/exclude` will be ignored. - If `str`/`list` used, The filter names will be `Filter%d` - A dict is used when keys are filter names and values are expressions \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"keep\" : True , \"ncores\" : config . misc . ncores , \"includes\" : None , \"excludes\" : None , \"tmpdir\" : config . path . tmpdir , \"args\" : {}, } script = \"file://../scripts/bcftools/BcftoolsFilter.py\"","title":"biopipen.namespaces.bcftools"},{"location":"api/source/biopipen.namespaces.bed/","text":"SOURCE CODE biopipen.namespaces. bed DOCS \"\"\"Tools to handle BED files\"\"\" from ..core.proc import Proc from ..core.config import config class BedLiftOver ( Proc ): DOCS \"\"\"Liftover a BED file using liftOver Input: inbed: The input BED file Output: outbed: The output BED file Envs: liftover: The path to liftOver chain: The map chain file for liftover \"\"\" input = \"inbed:file\" output = \"outbed:file:{{in.inbed | basename}}\" envs = { \"liftover\" : config . exe . liftover , \"chain\" : config . path . liftover_chain , } lang = config . lang . bash script = \"file://../scripts/bed/BedLiftOver.sh\"","title":"biopipen.namespaces.bed"},{"location":"api/source/biopipen.namespaces.csv/","text":"SOURCE CODE biopipen.namespaces. csv DOCS \"\"\"Tools to deal with csv/tsv files\"\"\" from ..core.proc import Proc class BindRows ( Proc ): DOCS \"\"\"Bind rows of input files\"\"\" input = \"infiles:files\" output = \"outfile:file:{{in.infiles[0] | stem}}.bound{{in.infiles[0] | ext}}\" script = \"\"\" outfile={{out.outfile | quote}} head -n 1 {{in.infiles[0] | quote}} > $outfile { % f or infile in in.infiles %} tail -n +2 {{infile | quote}} >> $outfile { % e ndfor %} \"\"\"","title":"biopipen.namespaces.csv"},{"location":"api/source/biopipen.namespaces.gene/","text":"SOURCE CODE biopipen.namespaces. gene DOCS \"\"\"Gene related processes\"\"\" from ..core.proc import Proc from ..core.config import config class GeneNameConversion ( Proc ): DOCS \"\"\"Convert gene names back and forth using MyGeneInfo Input: infile: The input file with original gene names Output: outfile: The output file with converted gene names Envs: inopts: Options to read `in.infile` for `pandas.read_csv()` See https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html outopts: Options to write `out.outfile` for `pandas.to_csv()` See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html notfound: What to do if a conversion cannot be done. use-query: Ignore the conversion and use the original name skip: Ignore the conversion and skip the entire row in input file error: Report error genecol: The index (0-based) or name of the column where genes are present output: How to output keep: Keep the original name column and add new converted columns drop: Drop the original name column, and add the converted names replace: Drop the original name column, and insert the converted names at the original position only: Only keep the query and the converted name columns infmt: What's the original gene name format Available fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields outfmt: What's the target gene name format species: Limit gene query to certain species. Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . python envs = { \"inopts\" : { \"sep\" : \" \\t \" , \"index_col\" : False }, \"outopts\" : { \"sep\" : \" \\t \" , \"index\" : False }, \"notfound\" : \"error\" , \"genecol\" : 0 , \"output\" : \"keep\" , \"infmt\" : [ \"symbol\" , \"alias\" ], \"outfmt\" : \"symbol\" , \"species\" : \"human\" , } script = \"file://../scripts/gene/GeneNameConversion.py\"","title":"biopipen.namespaces.gene"},{"location":"api/source/biopipen.namespaces.gsea/","text":"SOURCE CODE biopipen.namespaces. gsea DOCS \"\"\"Gene set enrichment analysis\"\"\" from ..core.proc import Proc from ..core.config import config class GSEA ( Proc ): DOCS \"\"\"Gene set enrichment analysis Need `devtools::install_github(\"GSEA-MSigDB/GSEA_R\")` Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample `[Group]`: The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. `clscol`: If not provided, will use `envs.clscol` `doc.string`: Documentation string used as a prefix to name result files. If not provided, will use `envs['doc.string']` Output: outdir: The output directory Envs: inopts: The options for `read.table()` to read the input file If `rds` will use `readRDS()` metaopts: The options for `read.table()` to read the meta file clscol: The column of the metafile determining the classes doc.string: Documentation string used as a prefix to name result files Other configs passed to `GSEA()` directly \"\"\" input = \"infile:file, metafile:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.infile | stem}}.gsea\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"metaopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"clscol\" : None , \"doc.string\" : \"gsea_result\" , } script = \"file://../scripts/gsea/GSEA.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/GSEA.svelte\" } class PreRank ( Proc ): DOCS \"\"\"PreRank the genes for GSEA analysis Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample `[Group]`: The groups/classes of the samples configfile: The configuration file in TOML format to specify some envs. `clscol`: If not provided, will use `envs.clscol` `classes`: Defines pos and neg labels. If not provided, use will `envs.classes`. Output: outfile: The rank file with 1st column the genes, and the rest the ranks for different class pairs provided by `envs.classes` or `in.configfile` Envs: inopts: Options for `read.table()` to read `in.infile` metaopts: Options for `read.table()` to read `in.metafile` method: The method to do the preranking. Supported: `s2n(signal_to_noise)`, `abs_s2n(abs_signal_to_noise)`, `t_test`, `ratio_of_classes`, `diff_of_classes` and `log2_ratio_of_classes`. clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. `[\"CASE\", \"CNTRL\"]`), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. `[[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]]`) \"\"\" input = \"infile:file, metafile:file, configfile:file\" output = \"outfile:file:{{in.infile | stem}}.rank\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"metaopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"method\" : \"s2n\" , \"clscol\" : None , \"classes\" : None , } script = \"file://../scripts/gsea/PreRank.R\" class FGSEA ( Proc ): DOCS \"\"\"Gene set enrichment analysis using `fgsea` Need `devtools::install_github(\"ctlab/fgsea\")` Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample `[Group]`: The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. `clscol`: If not provided, will use `envs.clscol` `classes`: Defines pos and neg labels. If not provided, use will `envs.classes`. Output: outdir: The output directory Envs: inopts: The options for `read.table()` to read the input file If `rds` will use `readRDS()` metaopts: The options for `read.table()` to read the meta file method: The method to do the preranking. Supported: `s2n(signal_to_noise)`, `abs_s2n(abs_signal_to_noise)`, `t_test`, `ratio_of_classes`, `diff_of_classes` and `log2_ratio_of_classes`. clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. `[\"CASE\", \"CNTRL\"]`), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. `[[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]]`) top: Do gsea table and enrich plot for top N pathways. If it is < 1, will apply it to `padj` `<rest>`: Rest arguments for `fgsea()` \"\"\" input = \"infile:file, metafile:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.infile | stem}}.fgsea\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"metaopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"method\" : \"s2n\" , \"clscol\" : None , \"classes\" : None , \"top\" : 20 , \"ncores\" : config . misc . ncores , \"minSize\" : 10 , \"maxSize\" : 100 , \"eps\" : 0 , } script = \"file://../scripts/gsea/FGSEA.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/FGSEA.svelte\" } class Enrichr ( Proc ): DOCS \"\"\"Gene set enrichment analysis using Enrichr Need `devtools::install_github(\"wjawaid/enrichR\")` Input: infile: The gene list file. You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output: outdir: The output directory Envs: inopts: Options for `read.table()` to read `in.infile` genecol: Which column has the genes (0-based index or column name) dbs: The databases to enrich against. See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries \"\"\" input = \"infile:file\" output = \"outdir:dir:{{in.infile | stem}}.enrichr\" lang = config . lang . rscript envs = { \"inopts\" : {}, \"genecol\" : 0 , \"genename\" : \"symbol\" , \"dbs\" : [ \"KEGG_2021_Human\" ], } script = \"file://../scripts/gsea/Enrichr.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/Enrichr.svelte\" }","title":"biopipen.namespaces.gsea"},{"location":"api/source/biopipen.namespaces/","text":"SOURCE CODE biopipen. namespaces DOCS","title":"biopipen.namespaces"},{"location":"api/source/biopipen.namespaces.misc/","text":"SOURCE CODE biopipen.namespaces. misc DOCS \"\"\"Misc processes\"\"\" from ..core.proc import Proc from ..core.config import config class File2Proc ( Proc ): DOCS \"\"\"Accept a file and pass it down with a symbolic link Input: infile: The input file Output: outfile: The output symbolic link to the input file \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" script = \"\"\" rm -f {{out.outfile | quote}} ln -s {{in.infile | quote}} {{out.outfile | quote}} \"\"\" class Glob2Dir ( Proc ): DOCS \"\"\"Create symbolic links in output directory for the files given by the glob pattern\"\"\" input = \"pattern:var\" output = \"outdir:dir:from_glob\" script = \"\"\" for infile in {{in.pattern}}; do if [[ -e $infile ]]; then ln -s $(realpath $infile) \"{{out.outdir}}/$(basename $infile)\"; fi done \"\"\" class Config2File ( Proc ): DOCS \"\"\"Write a configurationn in string to a configuration file Requires python package `rtoml` Input: config: A string representation of configuration name: The name for output file. Will be `config` if not given Output: outfile: The output file with the configuration Envs: infmt: The input format. `json` or `toml`. outfmt: The output format. `json` or `toml`. \"\"\" input = \"config:var, name:var\" output = ( \"outfile:file:{{(in.name or 'config') | slugify}}.{{envs.outfmt}}\" ) envs = { \"infmt\" : \"toml\" , \"outfmt\" : \"toml\" } lang = config . lang . python script = \"file://../scripts/misc/Config2File.py\" class Str2File ( Proc ): DOCS \"\"\"Write the given string to a file Input: str: The string to write to file name: The name of the file If not given, use `envs.name` Output: outfile: The output file Envs: name: The name of the output file \"\"\" input = \"str, name\" output = \"outfile:file:{{in.name}}\" lang = config . lang . python envs = { \"name\" : None } script = \"file://../scripts/misc/Str2File.py\"","title":"biopipen.namespaces.misc"},{"location":"api/source/biopipen.namespaces.plot/","text":"SOURCE CODE biopipen.namespaces. plot DOCS \"\"\"Plotting data\"\"\" from email import header from ..core.proc import Proc from ..core.config import config class VennDiagram ( Proc ): DOCS \"\"\"Plot Venn diagram Needs `ggVennDiagram` Input: infile: The input file for data If `envs.intype` is raw, it should be a data frame with row names as categories and only column as elements separated by comma (`,`) If it is `computed`, it should be a data frame with row names the elements and columns the categories. The data should be binary indicator (`0, 1`) indicating whether the elements are present in the categories. Output: outfile: The output figure file Envs: inopts: The options for `read.table()` to read `in.infile` intype: `raw` or `computed`. See `in.infile` devpars: The parameters for `png()` args: Additional arguments for `ggVennDiagram()` ggs: Additional ggplot expression to adjust the plot \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.venn.png\" lang = config . lang . rscript envs = { \"inopts\" : { \"row.names\" : - 1 , \"header\" : False }, \"intype\" : \"raw\" , \"devpars\" : { \"res\" : 100 , \"width\" : 1000 , \"height\" : 1000 }, \"args\" : {}, \"ggs\" : None , } script = \"file://../scripts/plot/VennDiagram.R\" class Heatmap ( Proc ): DOCS \"\"\"Plot heatmaps using `ComplexHeatmap` Examples: >>> pipen run plot Heatmap \\ >>> --in.infile data.txt \\ >>> --in.annofiles anno.txt \\ >>> --envs.args.row_names_gp 'r:fontsize5' \\ >>> --envs.args.column_names_gp 'r:fontsize5' \\ >>> --envs.args.clustering_distance_rows pearson \\ >>> --envs.args.clustering_distance_columns pearson \\ >>> --envs.args.show_row_names false \\ >>> --envs.args.row_split 3 \\ >>> --args.devpars.width 5000 \\ >>> --args.devpars.height 5000 \\ >>> --args.draw.merge_legends \\ >>> --envs.args.heatmap_legend_param.title AUC \\ >>> --envs.args.row_dend_reorder \\ >>> --envs.args.column_dend_reorder \\ >>> --envs.args.top_annotation \\ >>> 'r:HeatmapAnnotation( \\ >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) \\ >>> )' \\ >>> --envs.args.right_annotation \\ >>> 'r:rowAnnotation( \\ >>> AUC = anno_boxplot(as.matrix(data), outline = F) \\ >>> )' \\ >>> --args.globals \\ >>> 'fontsize8 = gpar(fontsize = 12); \\ >>> fontsize5 = gpar(fontsize = 8); \\ >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' \\ >>> --args.seed 8525 Input: infile: The data matrix file annofiles: The files for annotation data Output: outfile: The heatmap plot outdir: Other data of the heatmap Including RDS file of the heatmap, row clusters and col clusters. Envs: inopts: Options for `read.table()` to read `in.infile` anopts: Options for `read.table()` to read `in.annofiles` draw: Options for `ComplexHeatmap::draw()` args: Arguments for `ComplexHeatmap::Heatmap()` devpars: The parameters for device. seed: The seed globals: Some globals for the expression in `args` to be evaluated \"\"\" input = \"infile:file, annofiles:files\" output = \"\"\" { %- s et outdir = in.infile | stem | append: \".heatmap\" -%} outfile:file:{{outdir}}/{{outdir}}.png, outdir:dir:{{outdir}} \"\"\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"anopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"draw\" : {}, \"devpars\" : {}, \"args\" : { \"heatmap_legend_param\" : {}}, \"seed\" : None , \"globals\" : \"\" } script = \"file://../scripts/plot/Heatmap.R\"","title":"biopipen.namespaces.plot"},{"location":"api/source/biopipen.namespaces.rnaseq/","text":"SOURCE CODE biopipen.namespaces. rnaseq DOCS \"\"\"RNA-seq data analysis\"\"\" from ..core.proc import Proc from ..core.config import config class UnitConversion ( Proc ): DOCS \"\"\"Convert expression value units back and forth\"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . rscript envs = { \"infmt\" : \"matrix\" , # or rds \"inunit\" : None , \"outunit\" : None , \"refexon\" : config . ref . refexon , \"meanfl\" : None , \"inlog2p\" : False , \"outlog2p\" : False , } script = \"file://../scripts/rnaseq/UnitConversion.R\"","title":"biopipen.namespaces.rnaseq"},{"location":"api/source/biopipen.namespaces.scrna/","text":"SOURCE CODE biopipen.namespaces. scrna DOCS \"\"\"Tools to analyze single-cell RNA\"\"\" from ..core.proc import Proc from ..core.config import config class SeuratLoading ( Proc ): DOCS \"\"\"Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - `Sample` to specify the sample names. - `RNADir` to assign the path of the data to the samples The path will be read by `Read10X()` from `Seurat` Output: rdsfile: The RDS file with a list of Seurat object Envs: qc: The QC filter for each sample. This will be passed to `subset(obj, subset=<qc>)`. For example `nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5` \"\"\" input = \"metafile:file\" output = \"rdsfile:file:{{in.metafile | stem}}.seurat.RDS\" envs = { \"qc\" : \"\" } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratLoading.R\" class SeuratPreparing ( Proc ): DOCS \"\"\"Seurat - Loading and preparing data What will be done ? https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1) 1. All samples with be integrated as a single seurat object 2. QC 3. Normalization 4. Feature selection 5. Scaling 6. Linear dimensional reduction Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - `Sample` to specify the sample names. - `RNADir` to assign the path of the data to the samples The path will be read by `Read10X()` from `Seurat` Output: rdsfile: The RDS file with the Seurat object Note that the cell ids are preficed with sample names Envs: ncores: Number of cores to use \"\"\" input = \"metafile:file\" output = \"rdsfile:file:{{in.metafile | stem}}.seurat.RDS\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , } script = \"file://../scripts/scrna/SeuratPreparing.R\" # plugin_opts = { # \"report\": \"file://../reports/scrna/SeuratPreparing.svelte\" # } class SeuratClustering ( Proc ): DOCS \"\"\"Seurat - Determine the clusters Input: srtobj: The seurat object loaded by SeuratPreparing Output: rdsfile: The seurat object with cluster information groupfile: A groupfile with cells for future analysis Envs: FindClusters: Arguments to `FindClusters()` \"\"\" input = \"srtobj:file\" output = [ \"rdsfile:file:{{in.srtobj | stem}}.RDS\" , \"groupfile:file:{{in.srtobj | stem | replace: '.seurat', ''}}\" \".clusters.txt\" , ] lang = config . lang . rscript envs = { \"FindClusters\" : { \"resolution\" : 0.8 }} script = \"file://../scripts/scrna/SeuratClustering.R\" class GeneExpressionInvestigation ( Proc ): DOCS \"\"\"Investigation of expressions of genes of interest Input: srtobj: The seurat object loaded by SeuratPreparing groupfile: The group of cells with the first column the groups and rest the cells in each sample. Or the subset conditions using metadata of `srtobj` See `envs.group_subset` genefiles: The genes to show their expressions in the plots configfile: The configuration file (toml). See `envs` If not provided, use `envs` Output: outdir: The output directory with the plots Envs: group_subset: Is the `in.groupfile` subset conditions using metadata Or the groupfile as described. name: The name to name the job. Otherwise the stem of groupfile will be used target: Which sample to pull expression from could be multiple gopts: Options for `read.table()` to read the genefiles plots: Plots to generate for this case `boxplot`: - `use`: Which gene file to use (1-based) - `ncol`: Split the plot to how many columns? - `res`, `height` and `width` the parameters for `png()` `heatmap`: - `use`: Which gene file to use (1-based) - `res`, `height` and `width` the parameters for `png()` - other arguments for `ComplexHeatmap::Heatmap()` \"\"\" input = \"srtobj:file, groupfile:file, genefiles:files, configfile:file\" output = \"outdir:dir:{{in.configfile | stem0}}.exprs\" lang = config . lang . rscript order = 4 envs = { \"group_subset\" : False , \"name\" : None , \"target\" : None , \"gopts\" : {}, \"plots\" : {}, } script = \"file://../scripts/scrna/GeneExpressionInvistigation.R\" plugin_opts = { \"report\" : \"file://../reports/scrna/GeneExpressionInvistigation.svelte\" } class DimPlots ( Proc ): DOCS \"\"\"Seurat - Dimensional reduction plots Input: srtobj: The seruat object in RDS format configfile: A toml configuration file with \"cases\" If this is given, `envs.cases` will be overriden name: The name of the job, used in report Output: outdir: The output directory Envs: cases: The cases for the dim plots Keys are the names and values are the arguments to `Seurat::Dimplots` \"\"\" input = \"srtobj:file, configfile:file, name:var\" output = \"outdir:dir:{{in.srtobj | stem}}.dimplots\" lang = config . lang . rscript script = \"file://../scripts/scrna/DimPlots.R\" envs = { \"cases\" : { \"Ident\" : { \"group.by\" : \"ident\" } } } plugin_opts = { \"report\" : \"file://../reports/scrna/DimPlots.svelte\" , \"report_toc\" : False , } class MarkersFinder ( Proc ): DOCS \"\"\"Find markers between different groups of cells Input: srtobj: The seurat object loaded by SeuratLoading groupfile: The group of cells with first column the groups and rest the cells in each sample. casefile: The config file in TOML that looks like >>> [case1] >>> \"ident.1\" = \"ident.1\" >>> \"ident.2\" = \"ident.2\" >>> # other arguments for Seruat::FindMarkers() name: The name of the jobs, mosted used in report Output: outdir: The output directory for the markers Envs: ncores: Number of cores to use to parallelize the groups cases: The cases to find markers for. Values would be the arguments for `FindMarkers()` If \"ALL\" or \"ALL\" in the keys, the process will run for all groups in the groupfile. The other keys will be arguments to `FindMarkers` When `ident.2` is not given and there is only one group or more than two groups in groupfile, the rest cells in the object will be used as the control if it is `ident`, will igore the groupfile and find markers for all idents. dbs: The dbs to do enrichment analysis for significant markers See below for all librarys https://maayanlab.cloud/Enrichr/#libraries \"\"\" input = \"srtobj:file, groupfile:file, name:var, casefile:file\" output = \"outdir:dir:{{in.groupfile | stem0}}.markers\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"cases\" : { \"ALL\" : True }, \"dbs\" : [ \"GO_Biological_Process_2021\" , \"GO_Cellular_Component_2021\" , \"GO_Molecular_Function_2021\" , \"KEGG_2021_Human\" , ], } order = 5 script = \"file://../scripts/scrna/MarkersFinder.R\" plugin_opts = { \"report\" : \"file://../reports/scrna/MarkersFinder.svelte\" } class SCImpute ( Proc ): DOCS \"\"\"Impute the dropout values in scRNA-seq data. Input: infile: The input file for imputation Either a SeuratObject or a matrix of count/TPM groupfile: The file to subset the matrix or label the cells Could be an output from ImmunarchFilter Output: outfile: The output matrix Envs: infmt: The input format. Either `seurat` or `matrix \"\"\" input = \"infile:file, groupfile:file\" output = [ \"outfile:file:{{in.infile | stem | replace: '.seurat', ''}}.\" \"{{envs.outfmt}}\" ] lang = config . lang . rscript envs = { \"infmt\" : \"seurat\" , # or matrix \"outfmt\" : \"txt\" , # or csv, rds \"type\" : \"count\" , # or TPM \"drop_thre\" : 0.5 , \"kcluster\" : None , # or Kcluster \"ncores\" : config . misc . ncores , \"refgene\" : config . ref . refgene , } script = \"file://../scripts/scrna/SCImpute.R\" class SeuratFilter ( Proc ): DOCS \"\"\"Filtering cells from a seurat object Input: srtobj: The seurat object in RDS filterfile: The file with the filtering information Either a group file (rows cases for filtering, columns are samples or ALL for all cells with prefices), or config under `subsetting` section in TOML with keys and value that will be passed to `subset(...,subset = ...)` Output: out: The filtered seurat object in RDS if `envs.multicase` is False, otherwise the directory with the filtered seurat objects Envs: filterfmt: `auto`, `subset` or `grouping`. If `subset` then `in.filterfile` will be config in TOML, otherwise if `grouping`, it is a groupfile. See `in.filterfile`. If `auto`, test if there is `=` in the file. If so, it's `subset` otherwise `grouping` invert: Invert the selection? multicase: If True, multiple seurat objects will be generated. For `envs.filterfmt == \"subset\"`, each key-value pair will be a case, otherwise, each row of `in.filterfile` will be a case. \"\"\" input = \"srtobj:file, filterfile:file\" output = \"\"\" { %- i f envs.multicase -%} out:dir:{{in.filterfile | stem}}.seuratfiltered { %- e lif envs.filterfmt == \"subset\" -%} out:file:{{in.filterfile | read | toml_loads | list | first}}.RDS { %- e lse -%} out:file:{{in.filterfile | readlines | last | split | first}}.RDS { %- e ndif -%} \"\"\" envs = { \"filterfmt\" : \"auto\" , # subset or grouping \"invert\" : False , \"multicase\" : True , } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratFilter.R\"","title":"biopipen.namespaces.scrna"},{"location":"api/source/biopipen.namespaces.scrna_metabolic/","text":"SOURCE CODE biopipen.namespaces. scrna_metabolic DOCS \"\"\"Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape If you have clustering done somewhere else, you could use `replace_clustering()` See docs here https://pwwang.github.io/biopipen/pipelines/scrna_metabolic Reference: [1] Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Start Process: MetabolicInputs \"\"\" from pathlib import Path from typing import Any , Mapping from datar.all import tibble , if_else from pipen import Pipen from ..core.config import config from ..core.proc import Proc OPTIONS = { \"clustered\" : config . pipeline . scrna_metabolic . clustered } def build_processes ( options : Mapping [ str , Any ] = None ): DOCS \"\"\"Build processes for metabolic landscape analysis pipeline\"\"\" from .scrna import SeuratPreparing , SeuratFilter , SeuratClustering , SCImpute options = options or OPTIONS class MetabolicInputs ( Proc ): \"\"\"Input for the metabolic pathway analysis pipeline for scRNA-seq data Input: metafile: A metafile indicating the metadata Two columns are required: `Sample` and `RNADir` `Sample` should be the first column with unique identifiers for the samples and `RNADir` indicates where the expression matrice are. Currently only 10X data is supported subsetfile: A file with information to subset the cells. Each subset of cells will be treated separately. See `subsetting` of `in.config` groupfile: The group file to group the cells. Rows are groups, and columns are cell IDs (without sample prefices) from samples or a single column `ALL` but with all cell IDs with sample prefices. Or it can be served as a config file for subsetting the cells. See `grouping` of `in.config` gmtfile: The GMT file with the metabolic pathways config: String of configurations in TOML for the pipeline `grouping_name`: The name of the groupings. Default: `Group` `grouping`: How the cells should be grouped. If `\"Input\"`, use `in.groupfile` directly to group the cells else if `Idents`, use `Idents(srtobj)` as groups. Otherwise (`Config`), otherwise it is TOML config with `name => condition` pairs. The `condition` will be passed to `subset(srtobj, ...)` to get the subset of cells `subsetting`: Similar as `grouping`, indicating how to use `in.subsetfile` to subset the cells. Output: metafile: Soft link to `in.metafile` subsetfile: Soft link to `in.subsetfile` groupfile: Soft link to `in.groupfile` gmtfile: Soft link to `in.gmtfile` configfile: The config file with `in.config` saved \"\"\" input = \"\"\" metafile:file, subsetfile:file, groupfile:file, gmtfile:file, config:var \"\"\" output = \"\"\" { %- f or inkey, inval in in.items() -%} { %- i f inkey != \"config\" -%} {{- inkey}}:file:{{inval | str | basename -}}, { %- e ndif -%} { %- e ndfor -%} configfile:file:config.toml \"\"\" script = \"\"\" { % a ddfilter writefile %} def writefile(s, outfile): with open(outfile, \"w\") as f: f.write(s) { % e ndaddfilter %} { %- f or inkey, inval in in.items() -%} { %- i f inkey != \"config\" and inval %} ln -s {{inval | quote}} {{out[inkey] | quote -}} { % e lif inkey != \"config\" and not inval %} touch {{out[inkey] | quote -}} { %- e ndif -%} { %- e ndfor %} cat > {{out.configfile | quote}} <<EOF {{in.config | replace: \"`\", \" \\\\ `\"}} EOF \"\"\" class MetabolicSeuratPreparing ( SeuratPreparing ): if not options [ \"clustered\" ]: requires = MetabolicInputs class MetabolicSeuratClustering ( SeuratClustering ): requires = MetabolicSeuratPreparing class MetabolicCellSubsets ( SeuratFilter ): if options [ \"clustered\" ]: requires = MetabolicInputs input_data = lambda ch : tibble ( srtobj = ch . metafile , filterfile = if_else ( [ ssfile is None or Path ( ssfile ) . name == \"None\" for ssfile in ch . subsetfile ], ch . configfile , ch . subsetfile , ), ) else : requires = MetabolicSeuratClustering , MetabolicInputs input_data = lambda ch1 , ch2 : tibble ( srtobj = ch1 . rdsfile , filterfile = if_else ( [ ssfile is None or Path ( ssfile ) . name == \"None\" for ssfile in ch2 . subsetfile ], ch2 . configfile , ch2 . subsetfile , ), ) class MetabolicCellGroups ( Proc ): \"\"\"Group cells for metabolic landscape analysis Each group of cells will do the enrichment against the metabolic pathways \"\"\" requires = MetabolicCellSubsets , MetabolicInputs input = \"srtdir:file, groupfile:file, configfile:file\" output = \"outdir:dir:{{in.srtdir | stem}}\" lang = config . lang . rscript script = \"file://../scripts/scrna_metabolic/MetabolicCellGroups.R\" class MetabolicExprImputation ( SCImpute ): \"\"\"Impute the dropout values in scRNA-seq data.\"\"\" requires = MetabolicCellGroups input = \"srtdir:file\" output = \"outdir:dir:imputed\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"drop_thre\" : 0.5 } script = \"file://../scripts/scrna_metabolic/MetabolicExprImputation.R\" class MetabolicPrepareSCE ( Proc ): \"\"\"Prepare SingleCellExperiment objects\"\"\" requires = MetabolicExprImputation , MetabolicCellGroups , MetabolicInputs input = \"impdir:dir, srtdir:dir, gmtfile:file\" output = \"outfile:file:metabolic.sce.RDS\" lang = config . lang . rscript envs = { \"refexon\" : config . ref . refexon } script = \"file://../scripts/scrna_metabolic/MetabolicPrepareSCE.R\" class MetabolicExprNormalization ( Proc ): \"\"\"Normalize the expression data using deconvolution\"\"\" requires = MetabolicPrepareSCE , MetabolicInputs input = \"sceobj:file, configfile:file\" output = \"outfile:file:{{in.sceobj | stem}}.sce.RDS\" envs = { \"dropout\" : 0.75 , \"refexon\" : config . ref . refexon } lang = config . lang . rscript script = ( \"file://../scripts/scrna_metabolic/MetabolicExprNormalization.R\" ) class MetabolicPathwayActivity ( Proc ): \"\"\"Pathway activities for each group\"\"\" requires = MetabolicExprNormalization , MetabolicInputs input = \"sceobj:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.sceobj | stem}}.pathwayactivity\" envs = { \"ntimes\" : 5000 , \"ncores\" : config . misc . ncores , \"heatmap_devpars\" : { \"res\" : 100 , \"width\" : 1200 , \"height\" : 2000 }, \"violin_devpars\" : { \"res\" : 100 , \"width\" : 1000 , \"height\" : 550 }, } order = 1 lang = config . lang . rscript script = \"file://../scripts/scrna_metabolic/MetabolicPathwayActivity.R\" plugin_opts = { \"report\" : ( \"file://\" \"../reports/scrna_metabolic/MetabolicPathwayActivity.svelte\" ) } class MetabolicPathwayHeterogeneity ( Proc ): \"\"\"Pathway heterogeneity\"\"\" requires = MetabolicExprNormalization , MetabolicInputs input = \"sceobj:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.sceobj | stem}}.pathwayhetero\" lang = config . lang . rscript order = 2 envs = { \"select_pcs\" : 0.8 , \"pathway_pval_cutoff\" : 0.01 , \"ncores\" : config . misc . ncores , \"bubble_devpars\" : { \"res\" : 100 , \"width\" : 1200 , \"height\" : 700 } } script = ( \"file://../scripts/scrna_metabolic/MetabolicPathwayHeterogeneity.R\" ) plugin_opts = { \"report\" : ( \"file://../reports/scrna_metabolic/\" \"MetabolicPathwayHeterogeneity.svelte\" ) } class MetabolicFeatures ( Proc ): \"\"\"Inter-subset metabolic features - Enrichment analysis in details\"\"\" requires = MetabolicExprNormalization , MetabolicInputs input = \"sceobj:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.sceobj | stem}}.pathwayfeatures\" lang = config . lang . rscript order = 3 envs = { \"ncores\" : config . misc . ncores , \"fgsea\" : True , \"prerank_method\" : \"signal_to_noise\" , \"top\" : 10 , } script = \"file://../scripts/scrna_metabolic/MetabolicFeatures.R\" plugin_opts = { \"report\" : ( \"file://../reports/scrna_metabolic/MetabolicFeatures.svelte\" ) } class MetabolicFeaturesIntraSubsets ( Proc ): \"\"\"Intra-subset metabolic features - Enrichment analysis in details\"\"\" requires = MetabolicExprNormalization , MetabolicInputs input = \"sceobj:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.sceobj | stem}}.intras-pathwayfeatures\" lang = config . lang . rscript order = 4 envs = { \"ncores\" : config . misc . ncores , \"fgsea\" : True , \"prerank_method\" : \"signal_to_noise\" , \"top\" : 10 , } script = ( \"file://../scripts/scrna_metabolic/MetabolicFeaturesIntraSubsets.R\" ) plugin_opts = { \"report\" : ( \"file://../reports/scrna_metabolic/\" \"MetabolicFeaturesIntraSubsets.svelte\" ) } return MetabolicInputs def metabolic_landscape () -> Pipen : DOCS \"\"\"Build a pipeline for `pipen run` to run\"\"\" return Pipen ( name = \"metabolic-landscape\" , desc = \"Metabolic landscape analysis for scRNA-seq data\" ) . set_start ( build_processes ())","title":"biopipen.namespaces.scrna_metabolic"},{"location":"api/source/biopipen.namespaces.tcr/","text":"SOURCE CODE biopipen.namespaces. tcr DOCS \"\"\"Tools to analyze single-cell TCR sequencing data\"\"\" from ..core.defaults import SCRIPT_DIR from ..core.proc import Proc from ..core.config import config class ImmunarchLoading ( Proc ): DOCS \"\"\"Immuarch - Loading data Build based on immunarch 0.6.7 See https://immunarch.com/articles/v2_data.html for supported data formats Currently only 10x data format is supported Library `dplyr` is also required to manipulate the meta data. Input: metafile: The meta data of the samples A tab-delimited file Two columns are required: - `Sample` to specify the sample names. - `TCRDir` to assign the path of the data to the samples, and this column will be excluded as metadata. Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like `filtered_contig_annotations.csv`, which doesn't have any name information. Output: rdsfile: The RDS file with the data and metadata Envs: tmpdir: The temporary directory to link all data files. \"\"\" input = \"metafile:file\" output = \"rdsfile:file:{{in.metafile | stem}}.immunarch.RDS\" lang = config . lang . rscript envs = { \"tmpdir\" : config . path . tmpdir } script = \"file://../scripts/tcr/ImmunarchLoading.R\" class ImmunarchFilter ( Proc ): DOCS \"\"\"Immunarch - Filter data See https://immunarch.com/articles/web_only/repFilter_v3.html Input: immdata: The data loaded by `immunarch::repLoad()` filterfile: A config file in TOML. A dict of configurations with keys as the names of the group and values dicts with following keys. See `envs.filters` Output: outfile: The filtered `immdata` groupfile: Also a group file with first column the groups and other columns the cell barcodes in the samples Envs: merge: Merge the cells from the samples, instead of list cells for different samples. The cell ids will be preficed with the sample name, connected with `_`. The column name will be `ALL` instead. clonotype: Use clonotype (CDR3.aa) as the group. The name from `envs.filters` will be ignored filters: The filters to filter the data You can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by `immunarch::repFilter()`. There is one more method `by.count` supported to filter the count matrix. For `by.meta`, `by.repertoire`, `by.rep`, `by.clonotype` or `by.col` the values will be passed to `.query` of `repFilter()`. You can also use the helper functions provided by `immunarch`, including `morethan`, `lessthan`, `include`, `exclude` and `interval`. If these functions are not used, `include(value)` will be used by default. For `by.count`, the value of `filter` will be passed to `dplyr::filter()` to filter the count matrix. You can also specify `ORDER` to define the filtration order, which defaults to 0, higher `ORDER` gets later executed. For example: >>> {{ >>> \"Top20BM_Post\": {{ >>> \"by.meta\": {{\"Source\": \"BM\", \"Status\": \"Post\"}}, >>> \"by.count\": {{ >>> \"ORDER\": 1, \"filter\": \"TOTAL %in% TOTAL[1:20]\" >>> }} >>> }} >>> }} \"\"\" input = \"immdata:file, filterfile:file\" output = [ \"outfile:file:{{in.immdata | stem}}.RDS\" , \"groupfile:file:{{in.immdata | stem}}.groups.txt\" ] envs = { \"merge\" : False , \"clonotype\" : False , \"filters\" : {}, } lang = config . lang . rscript script = \"file://../scripts/tcr/ImmunarchFilter.R\" class ImmunarchBasic ( Proc ): DOCS \"\"\"Immunarch - Basic statistics and clonality See https://immunarch.com/articles/web_only/v3_basic_analysis.html Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory Envs: volume_by: Groupings to show clonotype volume (sizes) Multiple groups supported, for example: `volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}}` Or label the groups: `volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\", \"Sex\"]}}` If a list or a single variable is given, it will be changed into `{{\"Status\": \"Status\"}}` len_by: Groupings to show CDR3 length of both aa and nt count_by: Groupings to show clonotype counts per sample top_clone_marks: `.head` arguments of `repClonoality()` top_clone_by: Groupings when visualize top clones rare_clone_marks: `.bound` arguments of `repClonoality()` rare_clone_by: Groupings when visualize rare clones hom_clone_marks: `.clone.types` arguments of `repClonoality()` hom_clone_by: Groupings when visualize homeo clones overlap_methods: The methods used for `repOverlap()`, each will generate a heatmap. overlap_redim: Plot the samples with these dimension reduction methods \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.basic\" lang = config . lang . rscript envs = { # basic statistics \"volume_by\" : {}, \"len_by\" : {}, \"count_by\" : {}, # clonality \"top_clone_marks\" : [ 10 , 100 , 1000 , 3000 , 10000 ], \"top_clone_by\" : {}, \"rare_clone_marks\" : [ 1 , 3 , 10 , 30 , 100 ], \"rare_clone_by\" : {}, \"hom_clone_marks\" : dict ( Small = 0.0001 , Medium = 0.001 , Large = 0.01 , Hyperexpanded = 1 , ), \"hom_clone_by\" : {}, # overlapping \"overlap_methods\" : [ \"public\" ], \"overlap_redim\" : [ \"tsne\" , \"mds\" ], } script = \"file://../scripts/tcr/ImmunarchBasic.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/ImmunarchBasic.svelte\" } class ImmunarchAdvanced ( Proc ): DOCS \"\"\"Immunarch - Advanced analysis Including gene usage, diversity estimation, tracking clonotype changes and kmer/motif analysis Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory Envs: gu_by: Groupings to show gene usages Multiple groups supported, for example: `volume_by = {{0: \"Status\", 1: [\"Status\", \"Sex\"]}}` Or label the groups: `volume_by = {{\"Status\": \"Status\", \"Status_Sex\": [\"Status\",\"Sex\"]}}` If a list or a single variable is given, it will be changed into `{{\"Status\": \"Status\"}}` gu_top: How many top (ranked by total usage across samples) genes to show in the plots gua_methods: controls how the data is going to be preprocessed and analysed. One of js, cor, cosine, pca, mds, and tsne spect: `.quant` and `.col` for `spectratype()` for each sample div_methods: Methods to calculate diversities div_by: Groupings to show sample diversities raref_by: Groupings to show rarefactions tracking_target: and tracking_samples: The target and samples to tracking You can do multiple trackings. To do that, you need to specify a key for each tracking. It will use the target and samples under the same key. If samples from `tracking_samples` cannot be found, all samples will be used Other than the target supported by immunarch, you can also specify top shared clones. For example: `tracking_target = {{ \"top_4\": {{\"TOP\": 4}} }}` kmers: Arguments for kmer analysis. Keys are the K of mers. Values are parameters: - `head` specifies # of the most abundant kmers to visualise. - `position`: positions of bars: `stack`, `dodge` and `fill` - `log`: log-transformation of y-axis - `motif`: Method for motif analysis There can be multiple `head`s and `motif`s. If you do want multiple parameter sets for the same K, You can use a float number as the K. For example: `5.1` for K `5`. \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.advanced\" lang = config . lang . rscript envs = { # gene usage \"gu_by\" : {}, \"gu_top\" : 30 , # gene usage analysis \"gua_methods\" : [ \"js\" , \"cor\" ], # Spectratyping \"spect\" : [ dict ( quant = \"id\" , col = \"nt\" ), dict ( quant = \"count\" , col = \"aa+v\" )], # Diversity \"div_methods\" : [ \"div\" , \"gini.simp\" ], \"div_by\" : {}, \"raref_by\" : {}, # Clonotype tracking \"tracking_target\" : {}, \"tracking_samples\" : {}, # can specify order # Kmer analysis \"kmers\" : { 5 : { \"head\" : 10 , \"position\" : \"stack\" , \"log\" : False , \"motif\" : \"self\" } }, } script = \"file://../scripts/tcr/ImmunarchAdvanced.R\" order = 1 plugin_opts = { \"report\" : \"file://../reports/tcr/ImmunarchAdvanced.svelte\" } class CloneResidency ( Proc ): DOCS \"\"\"Identification of clone residency Typically, where the clones are located for the sample patient. Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory Envs: subject: The key of subject in metadata. The clone residency will be examined for this subject/patient group: The key of group in metadata. This usually marks the samples that you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. order: The order of the values in `group`. Early-ordered group will be used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the scatter plots will be drawn for pairs: B ~ A, C ~ B. \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.cloneov\" lang = config . lang . rscript envs = { \"subject\" : [], \"group\" : None , \"order\" : [], } script = \"file://../scripts/tcr/CloneResidency.R\" order = 2 plugin_opts = { \"report\" : \"file://../reports/tcr/CloneResidency.svelte\" } class Immunarch2VDJtools ( Proc ): DOCS \"\"\"Convert immuarch format into VDJtools input formats Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory containing the vdjtools input for each sample \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.vdjtools_input\" lang = config . lang . rscript script = \"file://../scripts/tcr/Immunarch2VDJtools.R\" class VJUsage ( Proc ): DOCS \"\"\"Circos-style V-J usage plot displaying the frequency of various V-J junctions. Input: infile: The input file, in vdjtools input format Output: outfile: The V-J usage plot Envs: vdjtools: The path to vdjtools vdjtools_patch: A patch for vdjtools \"\"\" input = \"infile:file\" output = \"outfile:file:{{ in.infile | stem | replace: '.vdjtools', '' }}.fancyvj.wt.png\" lang = config . lang . rscript envs = { \"vdjtools\" : config . exe . vdjtools , \"vdjtools_patch\" : str ( SCRIPT_DIR / \"tcr\" / \"vdjtools-patch.sh\" ), } order = 3 script = \"file://../scripts/tcr/VJUsage.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/VJUsage.svelte\" } class Attach2Seurat ( Proc ): DOCS \"\"\"Attach the clonal information to a Seurat object as metadata Input: immfile: The immunarch object in RDS sobjfile: The Seurat object file in RDS Output: outfile: The Seurat object with the clonal information as metadata Envs: prefix: The prefix to the barcodes. You can use placeholder like `{{Sample}}_` to use the meta data from the immunarch object metacols: Which meta columns to attach \"\"\" input = \"immfile:file, sobjfile:file\" output = \"outfile:file:{{in.sobjfile | basename}}\" lang = config . lang . rscript envs = { \"prefix\" : \" {Sample} _\" , \"metacols\" : [ \"Clones\" , \"Proportion\" , \"CDR3.aa\" ], } script = \"file://../scripts/tcr/Attach2Seurat.R\" class TCRClustering ( Proc ): DOCS \"\"\"Cluster the TCR clones by their CDR3 sequences With GIANA https://github.com/s175573/GIANA > Zhang, Hongyi, Xiaowei Zhan, and Bo Li. > \"GIANA allows computationally-efficient TCR clustering and multi-disease > repertoire classification by isometric transformation.\" > Nature communications 12.1 (2021): 1-11. Or ClusTCR https://github.com/svalkiers/clusTCR > Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, > ClusTCR: a Python interface for rapid clustering of large sets of CDR3 > sequences with unknown antigen specificity, > Bioinformatics, 2021. Input: immfile: The immunarch object in RDS Output: immfile: The immnuarch object in RDS with TCR cluster information clusterfile: The cluster file. Columns are CDR3.aa, TCR_Cluster, V.name, Sample heatmap: The heatmap of the samples, in terms of their shared TCR Clusters Envs: tool: The tool used to do the clustering, either GIANA or ClusTCR For GIANA, using TRBV mutations is not supported on_raw: Whether to run clustering on raw seq or the seq read and processed by immunarch python: The path of python with `GIANA`'s dependencies installed or with `clusTCR` installed. Depending on the `tool` you choose. tmpdir: The temporary directory to store the GIANA sources giana_source: The URLs for the source code of GIANA args: The arguments for the clustering tool For GIANA, they will be passed to `python GIAna.py` For ClusTCR, they will be passed to `clustcr.Clustering(...)` heatmap_meta: The metadata to show in the heatmap for each sample Current only support categorical/character metadata numbers_on_heatmap: Whether to show the numbers on the heatmap \"\"\" input = \"immfile:file\" output = [ \"immfile:file:{{in.immfile | basename}}\" , \"clusterfile:file:{{in.immfile | stem}}.clusters.txt\" , \"heatmap:file:{{in.immfile | stem}}.heatmap.png\" , ] lang = config . lang . rscript envs = { \"tool\" : \"GIANA\" , # or ClusTCR \"on_raw\" : True , \"python\" : config . lang . python , \"tmpdir\" : config . path . tmpdir , \"giana_source\" : { \"url\" : ( \"https://raw.githubusercontent.com/\" \"s175573/GIANA/master/GIANA4.1.py\" ), \"giana4\" : ( \"https://raw.githubusercontent.com/\" \"s175573/GIANA/master/GIANA4.py\" ), \"query\" : ( \"https://raw.githubusercontent.com/\" \"s175573/GIANA/master/query.py\" ), \"trbv\" : ( \"https://raw.githubusercontent.com/\" \"s175573/GIANA/master/Imgt_Human_TRBV.fasta\" ), }, \"args\" : {}, \"heatmap_meta\" : [], \"numbers_on_heatmap\" : True , } script = \"file://../scripts/tcr/TCRClustering.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/TCRClustering.svelte\" , \"report_toc\" : False , }","title":"biopipen.namespaces.tcr"},{"location":"api/source/biopipen.namespaces.vcf/","text":"SOURCE CODE biopipen.namespaces. vcf DOCS \"\"\"Tools to handle VCF files\"\"\" from ..core.proc import Proc from ..core.config import config class VcfLiftOver ( Proc ): DOCS \"\"\"Liftover a VCF file using GATK Input: invcf: The input VCF file Output: outvcf: The output VCF file Envs: gatk: The path to gatk4, which should be installed via conda chain: The map chain file for liftover tmpdir: Directory for temporary storage of working files args: Other CLI arguments for `gatk LiftoverVcf` \"\"\" input = \"invcf:file\" output = \"outvcf:file:{{in.invcf | basename}}\" envs = { \"gatk\" : config . exe . gatk4 , \"chain\" : config . path . liftover_chain , \"tmpdir\" : config . path . tmpdir , \"reffa\" : config . ref . reffa , \"args\" : {}, } lang = config . lang . bash script = \"file://../scripts/vcf/VcfLiftOver.sh\" class VcfFilter ( Proc ): DOCS \"\"\"Filter records in vcf file Input: invcf: The input vcf file, could be bgzipped. Output: outfile: The filtered vcf file. If `in.invcf` is bgzipped, then this will be bgzipped. Envs: filters: A dict of filters with keys the filter names. >>> # Typically >>> lambda variant: <expression> Things to notice 1. Filters should return `False` to get variant filtered out 2. See https://brentp.github.io/cyvcf2/docstrings.html#cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters `{\"QUAL\": 30}` 5. List of builtin filters. Specify them like: `{\"FILTER\": params}` `SNPONLY`: keeps only SNPs (`{\"SNPONLY\": False}` to filter SNPs out) `QUAL`: keeps variants with QUAL>=param (`{\"QUAL\": (30, False)}`) to keep only variants with QUAL<30 filter_descs: Descriptions for the filters. Will be saved to the header of the output vcf file helper: Some helper code for the filters keep: Keep the variants not passing the filters? \"\"\" input = \"invcf:file\" output = \"outfile:file:{{in.invcf | basename}}\" lang = config . lang . python envs = { \"filters\" : {}, \"keep\" : True , \"helper\" : \"\" , \"filter_descs\" : {}, } script = \"file://../scripts/vcf/VcfFilter.py\" class VcfIndex ( Proc ): DOCS \"\"\"Index VCF files. If they are already index, use the index files Input: infile: The input VCF file Output: outfile: The output VCF file (bgzipped) outidx: The index file of the output VCF file Envs: tabix: Path to tabix \"\"\" input = \"infile:file\" output = \"\"\" { %- i f in.infile.endswith(\".gz\") %} outfile:file:{{in.infile | basename}}, outidx:file:{{in.infile | basename | append: \".tbi\"}} { %- e lse -%} outfile:file:{{in.infile | basename | append: \".gz\"}}, outidx:file:{{in.infile | basename | append: \".gz.tbi\"}} { % e ndif -%} \"\"\" envs = { \"tabix\" : config . exe . tabix , } script = \"file://../scripts/vcf/VcfIndex.py\" class VcfDownSample ( Proc ): DOCS \"\"\"Down-sample VCF files to keep only a subset of variants in there Input: infile: The input VCF file Output: outfile: The output VCF file with subet variants Gzipped if `in.infile` is gzipped Envs: n: Fraction/Number of variants to keep If `n > 1`, it is the number. If `n <= 1`, it is the fraction. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" envs = { \"n\" : 0 } script = \"file://../scripts/vcf/VcfDownSample.sh\"","title":"biopipen.namespaces.vcf"},{"location":"api/source/biopipen.namespaces.web/","text":"SOURCE CODE biopipen.namespaces. web DOCS \"\"\"Get data from the web\"\"\" from ..core.proc import Proc from ..core.config import config class Download ( Proc ): DOCS \"\"\"Download data from URLs Input: url: The URL to download data from Output: outfile: The file downloaded \"\"\" input = \"url\" output = ( \"outfile:file:\" \"{{in.url | basename | replace: ' %2E ', '.' | slugify: separator='.'}}\" ) lang = config . lang . python envs = { \"tool\" : \"wget\" , # or aria2c, python \"wget\" : config . exe . wget , \"aria2c\" : config . exe . aria2c , \"args\" : {}, \"ncores\" : config . misc . ncores , } script = \"file://../scripts/web/Download.py\" class DownloadList ( Proc ): DOCS input = \"urlfile:file\" output = \"outdir:dir:{{in.urlfile | stem}}.downloaded\" lang = config . lang . python envs = { \"tool\" : \"wget\" , # or aria2c \"wget\" : config . exe . wget , \"aria2c\" : config . exe . aria2c , \"args\" : {}, \"ncores\" : config . misc . ncores , } script = \"file://../scripts/web/DownloadList.py\"","title":"biopipen.namespaces.web"},{"location":"pipelines/scrna_metabolic/","text":"scrna_metabolic Metabolic landscape analysis for single-cell RNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape Reference Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Run the pipeline pipen run scrna_metabolic metabolic_landscape [ options ] Inputs metafile : A metafile indicating the metadata Two columns are required: Sample and RNADir Sample should be the first column with unique identifiers for the samples and RNADir indicates where the expression matrice are. Currently only 10X data is supported You can also pass a Seurat with metadata attached. subsetfile : A file with information to subset the cells. Each subset of cells will be treated separately. See subsetting of in.config groupfile : The group file to group the cells. Rows are groups, and columns are cell IDs (without sample prefices) from samples or a single column ALL but with all cell IDs with sample prefices. Or it can be served as a config file for subsetting the cells. See grouping of in.config gmtfile : The GMT file with the metabolic pathways config : String of configurations in TOML for the pipeline grouping_name : The name of the groupings. Default: Group grouping : How the cells should be grouped. If \"Input\" , use in.groupfile directly to group the cells else if Idents , use Idents(srtobj) as groups. Otherwise ( Config ), it is TOML config with name => condition pairs. The condition will be passed to subset(srtobj, ...) to get the subset of cells subsetting : Similar as grouping , indicating how to use in.subsetfile to subset the cells. design : For intra-subset comparisons. For example: case1 = [<subset1>, <subset2>] A step-by-step example Prepare the seurat object Using the data from: Yost KE, Satpathy AT, Wells DK, Qi Y et al. Clonal replacement of tumor-specific T cells following PD-1 blockade. Nat Med 2019 Aug;25(8):1251-1259. PMID: 31359002 library ( Seurat ) # Download data (tcell rds and metadata) from # https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123813 count_file <- \"test_data/scrna_metabolic/GSE123813_bcc_scRNA_counts.txt.gz\" meta_file <- \"test_data/scrna_metabolic/GSE123813_bcc_all_metadata.txt.gz\" counts <- read.table ( count_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) metadata <- read.table ( meta_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) # Subset 1000 cells for jusb demo purpose counts = counts [, sample ( 1 : ncol ( counts ), 1000 )] metadata = metadata [ colnames ( counts ),] # Create seurat object seurat_obj = CreateSeuratObject ( counts = counts ) seurat_obj @ meta.data = cbind ( seurat_obj @ meta.data , metadata [ rownames ( seurat_obj @ meta.data ),] ) seurat_obj = NormalizeData ( seurat_obj ) all.genes <- rownames ( seurat_obj ) seurat_obj <- ScaleData ( seurat_obj , features = all.genes ) seurat_obj <- FindVariableFeatures ( object = seurat_obj ) seurat_obj <- RunPCA ( seurat_obj , features = VariableFeatures ( object = seurat_obj )) # Output exceeds the size limit. Open the full output data in a text editor # Warning message: # \"Feature names cannot have underscores ('_'), replacing with dashes ('-')\" # Centering and scaling data matrix # PC_ 1 # Positive: CALD1, EMP2, SPARC, CAV1, EMP1, ACTN1, KRT17, BGN, DSP, RAB13 # RND3, S100A16, KRT14, S100A14, S100A2, CD9, FHL2, IER3, GJB2, TRIM29 # TSC22D1, KRT15, SFN, FSTL1, DDR1, IGFBP7, JUP, PTRF, KRT5, FOSL1 # Negative: CD74, CRIP1, RARRES3, RGS1, LAT, CD69, NKG7, HLA-DPA1, TNFRSF4, CD27 # GZMA, HLA-DRB1, CXCR6, CTSW, GPR183, LDLRAD4, ICOS, HIST1H4C, GZMK, AC092580.4 # SLC9A3R1, CCR7, S100A4, HLA-DPB1, HMGB2, GBP5, SELL, CXCR3, LAG3, HLA-DRB5 # PC_ 2 # Positive: DSP, DSC3, SFN, SERPINB5, KRT17, S100A14, KRT15, KRT16, IRF6, TRIM29 # KRT14, LGALS7B, JUP, PERP, TACSTD2, GJB2, KRT5, KRT6B, DDR1, S100A2 # PKP1, CDH3, KRT6A, FXYD3, GJB3, MPZL2, CXADR, DSC2, DSG3, GRHL3 # Negative: COL1A2, COL3A1, LUM, COL6A1, MXRA8, FN1, CTSK, COL1A1, COL6A3, DCN # MMP2, COL6A2, PRRX1, FKBP10, TNFAIP6, FAP, PCOLCE, PDGFRB, NNMT, AEBP1 # C1S, CCDC80, SFRP2, RCN3, PDPN, SERPINF1, COL5A2, CTHRC1, COL5A1, COL12A1 # PC_ 3 # Positive: LYZ, TYROBP, SPI1, FCER1G, KYNU, C15orf48, BCL2A1, HLA-DRA, CD68, AIF1 # CST3, CTSZ, SERPINA1, CSF2RA, HLA-DRB5, SLC7A11, LST1, MS4A7, ALDH2, FAM49A # IFI30, HLA-DRB1, GPR157, PLEK, HLA-DPB1, IL1B, CD86, CCDC88A, HLA-DPA1, RNF144B # Negative: MT2A, MT1X, BGN, MT1E, COL6A2, COL1A2, COL6A1, COL5A2, MXRA8, DCN # COL12A1, COL3A1, COL1A1, LUM, MFAP2, PCOLCE, MT1F, MMP2, COL5A1, COL6A3 # PRRX1, C1S, AEBP1, CTSK, FAP, LAT, PDGFRB, RARRES3, THY1, EFEMP2 # GJB3, LYPD3, GRHL3, PVRL4, KRT16, TACSTD2, GJB5, SERPINB13, MPZL2, KRT23 # Negative: UBE2C, GTSE1, BIRC5, RRM2, CCNA2, TYMS, TOP2A, TK1, DLGAP5, MKI67 # PKMYT1, CENPA, KIFC1, CDCA5, UHRF1, ASF1B, AURKB, FAM111B, TROAP, CKAP2L # HJURP, ESCO2, FOXM1, CDK1, ZWINT, E2F2, CLSPN, HIST1H1B, CDT1, MCM10 # By default, the pipeline assumes the cells are not clustered # and it will do the clustering using the scrna.SeuratClustering process # # If you want to do the clustering yourself, remember to add following # # [pipeline.scrna_metabolic] # clustered = false # # to your `.biopipen.toml` either in your home directory or current directory. seurat_obj <- FindNeighbors ( seurat_obj , dims = 1 : 10 ) seurat_obj <- FindClusters ( seurat_obj , resolution = 0.5 ) head ( Idents ( seurat_obj )) # Computing nearest neighbor graph # Computing SNN # Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck # Number of nodes: 1000 # Number of edges: 30631 # Running Louvain algorithm... # Maximum modularity in 10 random starts: 0.8795 # Number of communities: 9 # Elapsed time: 0 seconds # bcc.su009.post.tcell_CCATTCGCAATCTACG 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG8 8 # Levels: # '0''1''2''3''4''5''6''7''8' # Check the meta.data head ( seurat_obj @ meta.data ) # orig.ident nCount_RNA nFeature_RNA patient treatment sort cluster UMAP1 UMAP2 RNA_snn_res.0.5 seurat_clusters # <fct> <dbl> <int> <chr> <chr> <chr> <chr> <dbl> <dbl> <fct> <fct> # bcc.su009.post.tcell_CCATTCGCAATCTACG bcc.su009.post.tcell 4242 1726 su009 post CD45+ CD3+ CD8_mem_T_cells -9.1816435 0.7484789 0 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC bcc.su004.pre.tcell 3159 1466 su004 pre CD45+ CD3+ CD8_ex_T_cells 4.1067562 3.3754938 0 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT bcc.su006.pre.tcell 3279 1401 su006 pre CD45+ CD3+ Tregs 0.4816457 11.9388428 1 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA bcc.su001.post.tcell 5057 2218 su001 post CD45+ CD3+ CD8_ex_T_cells 2.3045983 7.4856248 2 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA bcc.su007.pre.tcell 3701 1413 su007 pre CD45+ CD3+ CD8_mem_T_cells -1.9617293 5.5546365 1 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG bcc.su009.pre.tcell 3891 1593 su009 pre CD45+ CD3+ Tcell_prolif 5.2243228 -1.0460106 8 8 # save seurat object saveRDS ( seurat_obj , \"test_data/scrna_metabolic/seurat_obj.rds\" ) Prepare the pathway file A set of collected metabolic pathways can be found here: https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape/blob/master/Data/KEGG_metabolism.gmt Download and save it to test_data/scrna_metabolic/KEGG_metabolism.gmt Prepare the configuration file Save at test_data/scrna_metabolic/config.toml : # Set input data [MetabolicInputs.in] metafile = [ \"test_data/scrna_metabolic/seurat_obj.rds\" ] gmtfile = [ \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" ] config = [ \"\"\" name = \" Patient : su001 \" grouping = \" Idents \" grouping_name = \" Cluster \" [subsetting] post = \" treatment == 'post' & patient == 'su001'\" pre = \" treatment == 'pre' & patient == 'su001'\" [design] # we also want to do some intra-subset comparisons post_vs_pre = [\" post \", \" pre \"] \"\"\" , ] # you can have multiple cases # config = [<config1>, <config2>] Run the pipeline # Make sure `pipeline.scrna_metabolic.clustered` is `false` in # `./.biopipen.toml` or `~/.biopipen.toml` pipen run scrna_metabolic metabolic_landscape --config test_data/scrna_metabolic/config.toml Check out the results/reports The results can be found at ./metabolic-landscape_results/ , and reports can be found at ./metabolic-landscape_results/REPORTS . To check out the reports, open ./metabolic-landscape_results/REPORTS/index.html in your browser. There are 4 parts of the results: MetabolicPathwayActivity : The pathway activities for groups (defined by grouping in the configration) for each subset (defined by subsetting ) MetabolicPathwayHeterogeneity : Pathway heterogeneity for groups for each subset MetabolicFeatures : The pathway enrichment analysis in detail for each group against the rest of the groups in each subset (inter-subset). MetabolicFeaturesIntraSubsets : The pathway enrichment analysis in detail by the designs (defined by design in the configration) for each group. The designs are basically comparisons between subsets, and that'ss why this is called intra-subsets","title":"scrna_metabolic"},{"location":"pipelines/scrna_metabolic/#scrna_metabolic","text":"Metabolic landscape analysis for single-cell RNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape","title":"scrna_metabolic"},{"location":"pipelines/scrna_metabolic/#reference","text":"Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12.","title":"Reference"},{"location":"pipelines/scrna_metabolic/#run-the-pipeline","text":"pipen run scrna_metabolic metabolic_landscape [ options ]","title":"Run the pipeline"},{"location":"pipelines/scrna_metabolic/#inputs","text":"metafile : A metafile indicating the metadata Two columns are required: Sample and RNADir Sample should be the first column with unique identifiers for the samples and RNADir indicates where the expression matrice are. Currently only 10X data is supported You can also pass a Seurat with metadata attached. subsetfile : A file with information to subset the cells. Each subset of cells will be treated separately. See subsetting of in.config groupfile : The group file to group the cells. Rows are groups, and columns are cell IDs (without sample prefices) from samples or a single column ALL but with all cell IDs with sample prefices. Or it can be served as a config file for subsetting the cells. See grouping of in.config gmtfile : The GMT file with the metabolic pathways config : String of configurations in TOML for the pipeline grouping_name : The name of the groupings. Default: Group grouping : How the cells should be grouped. If \"Input\" , use in.groupfile directly to group the cells else if Idents , use Idents(srtobj) as groups. Otherwise ( Config ), it is TOML config with name => condition pairs. The condition will be passed to subset(srtobj, ...) to get the subset of cells subsetting : Similar as grouping , indicating how to use in.subsetfile to subset the cells. design : For intra-subset comparisons. For example: case1 = [<subset1>, <subset2>]","title":"Inputs"},{"location":"pipelines/scrna_metabolic/#a-step-by-step-example","text":"","title":"A step-by-step example"},{"location":"pipelines/scrna_metabolic/#prepare-the-seurat-object","text":"Using the data from: Yost KE, Satpathy AT, Wells DK, Qi Y et al. Clonal replacement of tumor-specific T cells following PD-1 blockade. Nat Med 2019 Aug;25(8):1251-1259. PMID: 31359002 library ( Seurat ) # Download data (tcell rds and metadata) from # https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123813 count_file <- \"test_data/scrna_metabolic/GSE123813_bcc_scRNA_counts.txt.gz\" meta_file <- \"test_data/scrna_metabolic/GSE123813_bcc_all_metadata.txt.gz\" counts <- read.table ( count_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) metadata <- read.table ( meta_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) # Subset 1000 cells for jusb demo purpose counts = counts [, sample ( 1 : ncol ( counts ), 1000 )] metadata = metadata [ colnames ( counts ),] # Create seurat object seurat_obj = CreateSeuratObject ( counts = counts ) seurat_obj @ meta.data = cbind ( seurat_obj @ meta.data , metadata [ rownames ( seurat_obj @ meta.data ),] ) seurat_obj = NormalizeData ( seurat_obj ) all.genes <- rownames ( seurat_obj ) seurat_obj <- ScaleData ( seurat_obj , features = all.genes ) seurat_obj <- FindVariableFeatures ( object = seurat_obj ) seurat_obj <- RunPCA ( seurat_obj , features = VariableFeatures ( object = seurat_obj )) # Output exceeds the size limit. Open the full output data in a text editor # Warning message: # \"Feature names cannot have underscores ('_'), replacing with dashes ('-')\" # Centering and scaling data matrix # PC_ 1 # Positive: CALD1, EMP2, SPARC, CAV1, EMP1, ACTN1, KRT17, BGN, DSP, RAB13 # RND3, S100A16, KRT14, S100A14, S100A2, CD9, FHL2, IER3, GJB2, TRIM29 # TSC22D1, KRT15, SFN, FSTL1, DDR1, IGFBP7, JUP, PTRF, KRT5, FOSL1 # Negative: CD74, CRIP1, RARRES3, RGS1, LAT, CD69, NKG7, HLA-DPA1, TNFRSF4, CD27 # GZMA, HLA-DRB1, CXCR6, CTSW, GPR183, LDLRAD4, ICOS, HIST1H4C, GZMK, AC092580.4 # SLC9A3R1, CCR7, S100A4, HLA-DPB1, HMGB2, GBP5, SELL, CXCR3, LAG3, HLA-DRB5 # PC_ 2 # Positive: DSP, DSC3, SFN, SERPINB5, KRT17, S100A14, KRT15, KRT16, IRF6, TRIM29 # KRT14, LGALS7B, JUP, PERP, TACSTD2, GJB2, KRT5, KRT6B, DDR1, S100A2 # PKP1, CDH3, KRT6A, FXYD3, GJB3, MPZL2, CXADR, DSC2, DSG3, GRHL3 # Negative: COL1A2, COL3A1, LUM, COL6A1, MXRA8, FN1, CTSK, COL1A1, COL6A3, DCN # MMP2, COL6A2, PRRX1, FKBP10, TNFAIP6, FAP, PCOLCE, PDGFRB, NNMT, AEBP1 # C1S, CCDC80, SFRP2, RCN3, PDPN, SERPINF1, COL5A2, CTHRC1, COL5A1, COL12A1 # PC_ 3 # Positive: LYZ, TYROBP, SPI1, FCER1G, KYNU, C15orf48, BCL2A1, HLA-DRA, CD68, AIF1 # CST3, CTSZ, SERPINA1, CSF2RA, HLA-DRB5, SLC7A11, LST1, MS4A7, ALDH2, FAM49A # IFI30, HLA-DRB1, GPR157, PLEK, HLA-DPB1, IL1B, CD86, CCDC88A, HLA-DPA1, RNF144B # Negative: MT2A, MT1X, BGN, MT1E, COL6A2, COL1A2, COL6A1, COL5A2, MXRA8, DCN # COL12A1, COL3A1, COL1A1, LUM, MFAP2, PCOLCE, MT1F, MMP2, COL5A1, COL6A3 # PRRX1, C1S, AEBP1, CTSK, FAP, LAT, PDGFRB, RARRES3, THY1, EFEMP2 # GJB3, LYPD3, GRHL3, PVRL4, KRT16, TACSTD2, GJB5, SERPINB13, MPZL2, KRT23 # Negative: UBE2C, GTSE1, BIRC5, RRM2, CCNA2, TYMS, TOP2A, TK1, DLGAP5, MKI67 # PKMYT1, CENPA, KIFC1, CDCA5, UHRF1, ASF1B, AURKB, FAM111B, TROAP, CKAP2L # HJURP, ESCO2, FOXM1, CDK1, ZWINT, E2F2, CLSPN, HIST1H1B, CDT1, MCM10 # By default, the pipeline assumes the cells are not clustered # and it will do the clustering using the scrna.SeuratClustering process # # If you want to do the clustering yourself, remember to add following # # [pipeline.scrna_metabolic] # clustered = false # # to your `.biopipen.toml` either in your home directory or current directory. seurat_obj <- FindNeighbors ( seurat_obj , dims = 1 : 10 ) seurat_obj <- FindClusters ( seurat_obj , resolution = 0.5 ) head ( Idents ( seurat_obj )) # Computing nearest neighbor graph # Computing SNN # Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck # Number of nodes: 1000 # Number of edges: 30631 # Running Louvain algorithm... # Maximum modularity in 10 random starts: 0.8795 # Number of communities: 9 # Elapsed time: 0 seconds # bcc.su009.post.tcell_CCATTCGCAATCTACG 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG8 8 # Levels: # '0''1''2''3''4''5''6''7''8' # Check the meta.data head ( seurat_obj @ meta.data ) # orig.ident nCount_RNA nFeature_RNA patient treatment sort cluster UMAP1 UMAP2 RNA_snn_res.0.5 seurat_clusters # <fct> <dbl> <int> <chr> <chr> <chr> <chr> <dbl> <dbl> <fct> <fct> # bcc.su009.post.tcell_CCATTCGCAATCTACG bcc.su009.post.tcell 4242 1726 su009 post CD45+ CD3+ CD8_mem_T_cells -9.1816435 0.7484789 0 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC bcc.su004.pre.tcell 3159 1466 su004 pre CD45+ CD3+ CD8_ex_T_cells 4.1067562 3.3754938 0 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT bcc.su006.pre.tcell 3279 1401 su006 pre CD45+ CD3+ Tregs 0.4816457 11.9388428 1 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA bcc.su001.post.tcell 5057 2218 su001 post CD45+ CD3+ CD8_ex_T_cells 2.3045983 7.4856248 2 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA bcc.su007.pre.tcell 3701 1413 su007 pre CD45+ CD3+ CD8_mem_T_cells -1.9617293 5.5546365 1 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG bcc.su009.pre.tcell 3891 1593 su009 pre CD45+ CD3+ Tcell_prolif 5.2243228 -1.0460106 8 8 # save seurat object saveRDS ( seurat_obj , \"test_data/scrna_metabolic/seurat_obj.rds\" )","title":"Prepare the seurat object"},{"location":"pipelines/scrna_metabolic/#prepare-the-pathway-file","text":"A set of collected metabolic pathways can be found here: https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape/blob/master/Data/KEGG_metabolism.gmt Download and save it to test_data/scrna_metabolic/KEGG_metabolism.gmt","title":"Prepare the pathway file"},{"location":"pipelines/scrna_metabolic/#prepare-the-configuration-file","text":"Save at test_data/scrna_metabolic/config.toml : # Set input data [MetabolicInputs.in] metafile = [ \"test_data/scrna_metabolic/seurat_obj.rds\" ] gmtfile = [ \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" ] config = [ \"\"\" name = \" Patient : su001 \" grouping = \" Idents \" grouping_name = \" Cluster \" [subsetting] post = \" treatment == 'post' & patient == 'su001'\" pre = \" treatment == 'pre' & patient == 'su001'\" [design] # we also want to do some intra-subset comparisons post_vs_pre = [\" post \", \" pre \"] \"\"\" , ] # you can have multiple cases # config = [<config1>, <config2>]","title":"Prepare the configuration file"},{"location":"pipelines/scrna_metabolic/#run-the-pipeline_1","text":"# Make sure `pipeline.scrna_metabolic.clustered` is `false` in # `./.biopipen.toml` or `~/.biopipen.toml` pipen run scrna_metabolic metabolic_landscape --config test_data/scrna_metabolic/config.toml","title":"Run the pipeline"},{"location":"pipelines/scrna_metabolic/#check-out-the-resultsreports","text":"The results can be found at ./metabolic-landscape_results/ , and reports can be found at ./metabolic-landscape_results/REPORTS . To check out the reports, open ./metabolic-landscape_results/REPORTS/index.html in your browser. There are 4 parts of the results: MetabolicPathwayActivity : The pathway activities for groups (defined by grouping in the configration) for each subset (defined by subsetting ) MetabolicPathwayHeterogeneity : Pathway heterogeneity for groups for each subset MetabolicFeatures : The pathway enrichment analysis in detail for each group against the rest of the groups in each subset (inter-subset). MetabolicFeaturesIntraSubsets : The pathway enrichment analysis in detail by the designs (defined by design in the configration) for each group. The designs are basically comparisons between subsets, and that'ss why this is called intra-subsets","title":"Check out the results/reports"}]}